## biological information Stanford


Nick Shea has developed the most sophisticated teleosemantic treatment of genes to date (see Shea 2007a,b, 2011, 2013).

One way of developing these ideas is to focus on the evolved functions of the genetic machinery as a whole (Godfrey-Smith 1999). Carl Bergstrom and Martin Rosvall take this approach, emphasising the adapted, impressively engineered features of intergenerational gene flow in developing their ‚Äútransmission sense of information‚Äù. Bergstron and Rosvall point out that intergeneration gene flow is structured so as to make possible the transmission of arbitrary sequences (so the message is relatively unconstrained by the medium); that information is stored compactly and stably; that the bandwidth is large and indefinitely extendable. DNA sequences are reliably replicable with very high fidelity, so transmission is accurate, yet the mapping between DNA and amino acid sequence seems optimised to reduce the impact of those errors that do occur


## endocrine pipelines
Neuroendocrine cells are cells that receive neuronal input (through neurotransmitters released by nerve cells or neurosecretory cells) and, as a consequence of this input, release messenger molecules (hormones) into the blood. In this way they bring about an integration between the nervous system and the endocrine system, a process known as neuroendocrine integration. An example of a neuroendocrine cell is a cell of the adrenal medulla (innermost part of the adrenal gland), which releases adrenaline to the blood


## Jekely hypothesis
Possible historical sequences 
We have discussed six categories. Each combines an  explana-
tory model   with a   function of nervous systems . What sort of 
historical   organization of   these   options   might   exist?  One 
possibility is that some single one of our six options was the 
first or the most important factor in early nervous system 
evolution. Claims of this kind have been made, or in some 
cases implied, in a number of earlier discussions. For example, 
Je  ÃÅkely [6], described a historical sequence in which behaviour 
(especially locomotion) is central to early nervous system 
evolution, and an IO pattern of explanation is applied. Je  ÃÅ kely‚Äôs 
hypothesis is a modern version of scenarios sketched also by 
Parker [3] and Mackie [4,5], with a focus on ciliary loco-
motion. In Je  ÃÅkely‚Äôs model, precursors of nervous systems 
arose to improve control of ciliary locomotion by means of 
division of labour and economies of scale.  Consider, for 
example, the non-neural control of swimming in a sponge 
larva. Here sensory mechanisms influence the activity of 
cilia on the same cell, thereby steering the whole larva. This 
way of connecting sensory and motor capacities is notably 
inefficient, as every motor component needs its own sensor. 
It would be more efficient for a small number of sensory 
cells to control a large bank of motor devices, and this is 
what the advent of neurons makes possible. So one plausible 
account of the origin of nervous systems focuses on the effi-
cient control   of locomotion by sensory mechanisms in   a 
ciliated swimming stage of an early metazoan: this is an   IO 
behaviour   hypothesis




#s LEVELS CRAVER STANFORD

levels of mechanisms are defined locally within a multilevel mechanism: one item is at a lower level of mechanisms than another when the first item is a part of the second and when the first item is organized (spatially, temporally, and actively) with the other components such that together they realize the second item. Thus, the mechanism of spatial memory has multiple levels, some of which include organs such as the hippocampus generating a spatial map, some of which involve the cellular interactions that underlie map generation, and some of which involve the molecular mechanisms that underlie those cellular interactions (Craver 2007)



## NEGATION IN NLP Roxanaoxanaoxanaoxanaa Petcu, Samarth Bhargav, Maarten de Rijke, Evangelos Kanoulas
University of Amsterdam, Amsterdam, The Netherlands
{r.m.petcu, s.bhargav, m.derijke, e.kanoulas}@uva.nl
Abstract
Understanding and solving complex reason-
ing tasks is vital for addressing the informa-
tion needs of a user. Although dense neural
models learn contextualised embeddings, they
still underperform on queries containing nega-
tion. To understand this phenomenon, we study
negation in both traditional neural information
retrieval and LLM-based models. We (1) in-
troduce a taxonomy of negation that derives
from philosophical, linguistic, and logical def-
initions; (2) generate two benchmark datasets
that can be used to evaluate the performance of
neural information retrieval models and to fine-
tune models for a more robust performance on
negation; and (3) propose a logic-based classi-
fication mechanism that can be used to analyze
the performance of retrieval models on existing
datasets. Our taxonomy produces a balanced
data distribution over negation types, providing
a better training setup that leads to faster con-
vergence on the NevIR dataset. Moreover, we
propose a classification schema that reveals the
coverage of negation types in existing datasets,
offering insights into the factors that might af-
fect the generalization of fine-tuned models on
negation.

## G≈Çadziejewski. cf thousand brains theory
I propose that the function of a representation within a representational  mechanism is action guidance (see Anderson & Rosenberg, 2008; Bickhard,  1999, 2004; Mi≈Çkowski, 2013). 
#

## Sparse coding of chromatic natural images recovers universals in color naming and unique hues
Alexander Belsten; Paxon Frady; Bruno Olshausen
 Author Affiliations & Notes
Journal of Vision July 2025, Vol.25, 2734. doi:https://doi.org/10.1167/jov.25.9.2734

Abstract
Understanding the transformation from cone activations to color appearance is a central question in vision research. Here, we study the color statistics of natural images and propose a model of this transformation based on principles of efficient coding. We show that this model replicates universals in color naming from the World Color Survey (WCS) (Kay and Cook, 2023) and aligns with unique hues (Hering, 1878). Whether or not the unique hues have a privileged status in perception has been subject to debate (Conway et al., 2023), and our model provides additional data and computational analyses that help address this question. Using simulated long-, medium-, and short-wavelength (LMS) cone activations in response to natural scenes, we compute a decorrelating transform that spheres the activations (i.e., achieves unit variance in all directions). Utilizing overcomplete, non-negative sparse coding models, we derive optimal bases for representing the data distribution and analyze their tuning properties. The decorrelating transform aligns with the DKL color space (Derrington, Krauskopf, Lennie, 1984). The data distribution in this space exhibits asymmetrical, heavy-tailed structure within the chromatic plane, but not along the luminance axis. Adapting sparse coding models to this structure recovers bases that align with universal color categories identified by the WCS. Notably, the six basis-vector model aligns with the unique hues and replicates features of psychophysical characterizations of these hues, such as mutual exclusivity between opponent colors (e.g., red and green). This work demonstrates that a model based on principles of efficient coding can provide an account for the physiology of color processing and the psychology of color appearance. Specifically, sparse coding models adapted to the sparse structure of the LMS distribution recover bases aligned with universal color categories and the unique hues, supporting previous reasoning that the unique hues are a well-suited basis for describing color appearance.


## 2025-07-12 01:01:18

The neural coding framework for learning
generative models
1234567890():,;
Alexander Ororbia
1 ‚úâ & Daniel Kifer2
Neural generative models can be used to learn complex probability distributions from data, to
sample from them, and to produce probability density estimates. We propose a computa-
tional framework for developing neural generative models inspired by the theory of predictive
processing in the brain. According to predictive processing theory, the neurons in the brain
form a hierarchy in which neurons in one level form expectations about sensory inputs from
another level. These neurons update their local models based on differences between their
expectations and the observed signals. In a similar way, artiÔ¨Åcial neurons in our generative
models predict what neighboring neurons will do, and adjust their parameters based on how
well the predictions matched reality. In this work, we show that the neural generative models
learned within our framework perform well in practice across several benchmark datasets and
metrics and either remain competitive with or signiÔ¨Åcantly outperform other generative
models with similar functionality (such as the variational auto-encoder).

## 2025-07-12 01:01:30

The neural coding framework for learning
generative models
1234567890():,;
Alexander Ororbia
1 ‚úâ & Daniel Kifer2
Neural generative models can be used to learn complex probability distributions from data, to
sample from them, and to produce probability density estimates. We propose a computa-
tional framework for developing neural generative models inspired by the theory of predictive
processing in the brain. According to predictive processing theory, the neurons in the brain
form a hierarchy in which neurons in one level form expectations about sensory inputs from
another level. These neurons update their local models based on differences between their
expectations and the observed signals. In a similar way, artiÔ¨Åcial neurons in our generative
models predict what neighboring neurons will do, and adjust their parameters based on how
well the predictions matched reality. In this work, we show that the neural generative models
learned within our framework perform well in practice across several benchmark datasets and
metrics and either remain competitive with or signiÔ¨Åcantly outperform other generative
models with similar functionality (such as the variational auto-encoder).

## 2025-07-12 01:02:23

NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-29632-7
ne way to understand how the brain adapts to its
environment is to view it as a type of generative pattern-
creation model1, one that is engaged in a never-ending
process of self-correction, often without external teaching signals


## 2025-07-12 01:20:56
Path integration 
Path integration in desert ants, Cataglyphis fortis
MARTIN MULLER AND RUDIGER WEHNER
Department of Zoology, University of Zrifch, CH-8057 Zurich, Switzerland
Communicated by Thomas Eisner, March 3, 1988 (received for review November 20, 1987)

ABSTRACT Foraging desert ants, Cataglyphisfortis, con-
tinually keep track of their own positions relative to home-
i.e., integrate their tortuous outbound routes and return home

along straight (inbound) routes. By experimentally manipulat-
ing the ants' outbound trajectories we show that the ants solve

this path integration problem not by performing a true vector
summation (as a human navigator does) but by employing a
computationally simple approximation. This approximation is
characterized by small, but systematic, navigational errors that
helped us elucidate the ant's way of computing its mean home
vector.

## 2025-07-12 01:50:32
CIRRIMUTTA
The author discusses the Turing machine as part of the broader computer analogy for the brain, but clarifies that this analogy should not be taken literally.
Specifically, the author claims that:
‚Ä¢
Computers, including the concept from Turing, were invented to perform a very particular type of cognitive labor previously done by humans. This refers to the work of a "human computer"‚Äîa clerical laborer whose job was to perform arithmetical calculations in a "disciplined but unintelligent manner".
‚Ä¢
Therefore, what a digital computer essentially is, is a model of this very particular cognitive performance originally performed by human computers.
‚Ä¢
This model, however, disregards aspects of human cognition that do not involve the execution of coded instructions, similar to how a steam engine is a model of only one isolated kind of horse locomotion (e.g., hauling loads on flat surfaces). The behavior imitated by the computer was explicitly classified as "unintelligent" by Turing himself.
‚Ä¢
The comparison between the brain and a computer, though a "simplifying lens" or "filter" that allows neuroscientists to abstract away from the brain's immense complexity, leads to a problematic "literal interpretation" in neuroscience and philosophy of mind.
‚Ä¢
The author argues that this literal interpretation is a "fallacy of misplaced concreteness"‚Äîthe mistake of confusing the abstract model with concrete reality. This leads to the erroneous expectation that non-living machines could achieve human-like general intelligence and consciousness, by overlooking the "countless biological complexities" and "material particularities" that are disregarded by computational models but are likely essential for these capacities in living beings.

## 2025-07-12 01:53:16
CIRRIMUTTA
the invention of digital computers was intimately tied to the cognitive work previously performed by human "computers". The author highlights Turing's conception of the computing machine as an analogue to this specific human labor.
Here's what the sources say about this primal analogy:
‚Ä¢
Computers were invented to perform a particular job that was previously carried out by human laborers. This labor was "cognitive rather than muscular".
‚Ä¢
Turing's computing machine was designed to perform "all and only the cognitive work done by a human computer". These human computers were described as "clerical laborers whose job it was to perform arithmetical calculations".
‚Ä¢
Turing himself stated that "Electronic computers are intended to carry out any definite rule of thumb process which could have been done by a human operator working in a disciplined but unintelligent manner".
‚Ä¢
The author further quotes Turing, noting that the problems solvable by the machine are a "subset of" those problems that can be solved by "human clerical labour, working to fixed rules, and without understanding". This suggests that the original analogy specifically modeled aspects of human cognition that were mechanical and lacked deeper understanding.
‚Ä¢
This foundational understanding means that a "digital computer essentially is a model of a very particular cognitive performance originally performed by human computers", which specifically disregards "the aspects of human cognition that do not involve execution of coded instructions".

## 2025-07-12 21:23:45
DARWIN
With regard to the question of the means by which animals find their way home from a long distance, a striking account, in relation to man, will be found in the English translation of the Expedition to North Siberia, by Von Wrangell.4 He there describes the wonderful manner in which the natives kept a true course towards a particular spot, whilst passing for a long distance through hummocky ice, with incessant changes of direction, and with no guide in the heavens or on the frozen sea. He states (but I quote only from memory of many years standing) that he, an experienced surveyor, and using a compass, failed to do that which these savages easily effected. Yet no one will suppose that they possessed any special sense which is quite absent in us. We must bear in mind that neither a compass, nor the north star, nor any other such sign, suffices to guide a man to a particular spot through an intricate country, or through hummocky ice, when many deviations from a straight course are inevitable, unless the deviations are allowed for, or a sort of "dead reckoning" is kept. All men are able to do this in a greater or less degree, and the natives of Siberia apparently to a wonderful extent, though probably in an unconscious manner. This is effected chiefly, no doubt, by eyesight, but partly, perhaps, by the sense of muscular movement, in the same manner as a man with his eyes blinded can proceed (and some men much better than others) for a short distance in a nearly straight line, or turn at right angles, or back again. The manner in which the sense of direction is sometimes suddenly disarranged in very old and feeble persons, and the feeling of strong distress which, as I know, has been experienced by persons when they have suddenly found out that they have been proceeding in a wholly unexpected and wrong direction, leads to the suspicion that some part of the brain is specialised for the function of direction.5 Whether animals may not possess the faculty of keeping a dead reckoning of their course in a much more perfect degree than can man; or whether this faculty may not come into play on the commencement of a journey when an animal is shut up in a basket, I will not attempt to discuss, as I have not sufficient data.




## 2025-07-14 22:16:25

sudo apt install xclip xsel libnotify-bin

## 2025-07-14 22:16:50

Ruth Garrett Millikan proposes Pushmi-Pullyu Representations (PPRs) as a type of representation that inherently possesses **both a descriptive and a directive function simultaneously**. These representations are considered **more primitive** than mere conjunctions of pure descriptive and pure directive representations, as their employment is simpler and does not require sophisticated cognitive apparatus for practical inference.

Here are their functions in each domain discussed:

*   **Primitive Signals (e.g., Animal Calls, Bee Dances)**
    *   **Description**: Simple signals used by various animals, such as bird songs, rabbit thumps, or the dance of the honey bee.
    *   **Functions**:
        *   **Directive**: A proper function is to **guide mechanisms to produce its satisfaction condition**. For instance, a hen's food call causes chicks to **come to the place where the food is and nourish them**. The bee dance tells **where to go**. These signals function to **mediate the production of a certain kind of behavior** that varies as a direct function of environmental variation.
        *   **Descriptive**: A condition for the proper performance of the call is that there be food where the hen calls, making it descriptive of "here's food now". The bee dance tells **where the nectar is**.
        *   **Integration**: The call directly connects with action, **translating the shape of the environment into the shape of a certain kind of conforming action**. The descriptive and directive contents can be different, as in the hen's call ("here's food now" vs. "come here now and eat!").

*   **Perceptual Representations (Affordances)**
    *   **Description**: Percepts are states of the organism that vary directly according to certain variations in the distal environment, representing "affordances" or opportunities for action, as suggested by James J. Gibson.
    *   **Functions**:
        *   **Descriptive**: The perceived layout of one's distal environment is a representation of **how things are arranged**.
        *   **Directive**: It is also a representation of **possible ways of moving within that environment** (e.g., ways of passing through, climbing up, paths to walk on). These serve a function only if and when acted upon, as there is no reason to represent what can be done unless it sometimes effects its being done.
        *   **Integration**: Action towards fulfillment is **directly guided by the percept**, with variations in the environment (and thus the percept) translating directly into variations in the perceiver's movement.

*   **Imitative Behaviors**
    *   **Description**: Primitive mechanisms of imitation seen in "monkey see monkey do cells" in monkeys and early imitative behaviors in children (e.g., imitating facial expressions).
    *   **Functions**:
        *   **Descriptive (Picturing)**: These PPRs **picture what another is doing**.
        *   **Directive**: They simultaneously serve to **direct what the self is to do**.
        *   **Integration**: The act of picturing what one might do and intending to do it may be **two sides of numerically the same representing coin**, as imagining an action and actually performing it involve, in part, the same dedicated brain areas.

*   **Human Intentions**
    *   **Description**: Inner representations of what one is definitely going to do.
    *   **Functions**:
        *   **Directive**: They serve to **direct action**, performing their proper functions when they issue in the intended actions.
        *   **Descriptive**: They serve to **describe one's future** so that one can plan around it. A person cannot sincerely intend to do something without believing they will do it.
        *   **Integration**: If intentions are PPRs, their **dual nature is a biological or neurological truth**, not just a conceptual one. Unlike some other PPRs, the **contents of the directive and descriptive aspects of intentions coincide**.

*   **Social Norms and Roles**
    *   **Description**: Primitive representations of social norms and roles, such as "what one does," "what women do," or "what teachers do". These are considered an "essential glue" for human societies, helping to stabilize coordinative behaviors and expectations.
    *   **Functions**:
        *   **Descriptive**: These PPRs **describe what is understood to be "The Moral Order,"** an order taken as totally objective and real. They coordinate expectations.
        *   **Directive (Prescriptive)**: They simultaneously **prescribe how one should behave**, acting as a mechanism for stabilizing and spreading coordinative patterns of behavior.
        *   **Integration**: These thoughts are **undifferentiated between descriptive and directive**, grasped as simultaneously describing and prescribing. They help stabilize coordinative behaviors and coordinate expectations.

*   **Human Language (PP Mood)**
    *   **General Declarative Pattern (PP Mood)**
        *   **Description**: The declarative syntactic pattern in human language can have more than one function; sometimes it is descriptive, and sometimes it expresses a "PP mood".
        *   **Functions**:
            *   **Descriptive**: It produces **true expectations**.
            *   **Directive (Prescriptive)**: It produces **coincident behaviors**.
            *   **Integration**: Both functions are **explicit, literal, and fully conventional**, with the mood proliferating because it serves both functions at once. Examples include "No Johnny, we don't eat peas with our fingers" and "Married people only make love with each other".

    *   **Strict Orders**
        *   **Description**: Delivered in the English declarative pattern.
        *   **Functions**:
            *   **Directive**: Their function is to **impart an intention to a hearer directly**, without mediation through any decision-making process. For example, "You will report to the CO at 6 am sharp".

    *   **Statements Coordinating Group Intentions**
        *   **Description**: PPRs used to impart intentions to a group, serving a coordinating function.
        *   **Functions**:
            *   **Descriptive**: Imparts **expectations concerning the behavior of others** to each group member.
            *   **Directive**: Imparts **intentions concerning their own behavior** to each group member.
            *   **Integration**: Functions properly by imparting both intentions and expectations simultaneously, such as "The meeting will now come to order" when said by the chair.

    *   **Performative Utterances**
        *   **Description**: Many performative utterances (e.g., "The meeting is adjourned," "I now pronounce you man and wife") are in the PP mood. These are conventional or legal "moves" that, once performed, place restrictions on what may or must follow.
        *   **Functions**:
            *   **Directive**: Their **proper function is to channel behaviors that follow** so that they take certain forms and not others. They **produce conventional outcomes** and guide participants by producing inner PPRs.
            *   **Descriptive**: Their **proper function is to coordinate expectations accordingly**. They also have a function to **produce true beliefs that the named conventional move is in fact being performed**, acting as an "ordinary descriptive" that is true if uttered by an appropriate person in appropriate circumstances. There are two truth conditions: one based on the appropriate utterance, and another requiring the conventional outcome to actually ensue for coordinated expectations to be true.
            *   **Integration**: These utterances resolve the puzzle of seemingly creating facts ex nihilo because they **simultaneously direct subsequent behaviors and coordinate expectations**, while also describing the conventional act being performed. The utterance, in the right circumstances, **IS the move named**.

    *   **Thick Concepts**
        *   **Description**: Words expressing "thick concepts" like *rude*, *glorious*, or *graceful*.
        *   **Functions**:
            *   **Descriptive**: They **describe "attitudinal secondary qualities"** relative to a species or culture. Declarative sentences using these words properly function only if the objects of attribution have these attitudinal secondary qualities.
            *   **Directive**: Their proper function is to **produce in the hearer the relevant attitudes** towards the items described.
            *   **Integration**: These words **face two ways at once, describing their subjects and at the same time inducing attitudes** towards these subjects. The inner representations induced by these words may themselves be inner PPRs.

## 2025-07-15 11:37:25

5. Aristotle‚Äôs biology
Given the pride of place claimed by qualitative changes and also by formal and final causes in many11 of his works on the science of animals, the rarity of analogies between the workings of living organisms and mechanical devices is no surprise. Such analogies do exist, however, and are relevant here. In ch. 7 of On the Movement of Animals, Aristotle invokes a number of dunameis or capacities of the soul, including sense-perception, desire, and phantasia (very roughly, capac-ity for mental representation), to explain how movement is initiated in animals. As a result of the operations of those dunameis, certain alterations (alloi≈çseis, 701b18) or qualitative changes (not reducible to rearrangements of particles, as the atomists would have held) take place and lead to movement. The original impetus for movement is an almost imperceptible change in the region

## 2025-07-15 11:41:09

Paul Thagard's HOTCO model, represent high-level cognitive states (such as beliefs, goals, or decisions) as "units" with "valences" that spread through a network. These models suggest that human decision-making mechanisms use coherence algorithms that incorporate both cognitive and affective "links". Such models are characterized as "how-roughly" models, offering a preliminary understanding of how actual mechanisms work

## 2025-07-15 11:56:07

Three kinds of new mechanism
Arnon Levy
Received: 27 December 2011 / Accepted: 31 July 2012
 Springer Science+Business Media B.V. 2012
Abstract I distinguish three theses associated with the new mechanistic philosophy‚Äîconcerning causation, explanation and scientific methodology. Advocates of
each thesis are identified and relationships among them are outlined. I then look at
some recent work on natural selection and mechanisms. Framing that debate in
terms of different kinds of New Mechanism significantly affects what is at stake.

## 2025-07-15 11:56:17

I will describe three mechanist theses, which concern causation, explanation and
the relations among them.1 Mechanists have discussed other topics, such as levels
and reduction. But ideas about causation and explanation form the core, around
which the other topics are organized. The three theses are as follows:
1. Causal Mechanism (CM) is the view that causal relations, at least outside the
domain of fundamental physical phenomena, exist in virtue of underlying
mechanisms. CM rivals other accounts of causation, such as regularity views. It
is best seen as a contribution to metaphysics.
2. Explanatory Mechanism (EM) is a thesis about explanatory relevance: it states
that to explain a phenomenon, one must cite mechanistic information, i.e.
specify underlying parts and their organization. EM contrasts with other general
accounts of explanation, such as the Deductive-Nomological model.
3. Strategic Mechanism (SM) concerns the cognitive-epistemic power of mechanistic modelling and related scientific methods. It asserts that certain
phenomena are best handled mechanistically. Discussions of SM tend to
construe ‚Äòmechanism‚Äô fairly narrowly, in machine-like terms.

## 2025-07-15 12:43:21

Two Kinds of Information Processing in Cognition
Mark Sprevak1
# The Author(s) 2019
Abstract
What is the relationship between information and representation? Dating back at least
to Dretske (1981), an influential answer has been that information is a rung on a ladder
that gets one to representation. Representation is information, or representation is
information plus some other ingredient. In this paper, I argue that this approach
oversimplifies the relationship between information and representation. If one takes

current probabilistic models of cognition seriously, information is connected to repre-
sentation in a new way. It enters as a property of the represented content as well as a

property of the vehicles that carry that content. This offers a new, conceptually and
logically distinct way in which information and representation are intertwined in
cognition.

## 2025-07-15 12:43:25

Two Kinds of Information Processing in Cognition
Mark Sprevak1
# The Author(s) 2019
Abstract
What is the relationship between information and representation? Dating back at least
to Dretske (1981), an influential answer has been that information is a rung on a ladder
that gets one to representation. Representation is information, or representation is
information plus some other ingredient. In this paper, I argue that this approach
oversimplifies the relationship between information and representation. If one takes

current probabilistic models of cognition seriously, information is connected to repre-
sentation in a new way. It enters as a property of the represented content as well as a

property of the vehicles that carry that content. This offers a new, conceptually and
logically distinct way in which information and representation are intertwined in
cognition.


## 2025-07-15 Ramsey 

The Hard Problem of Content is Neither William Max Ramsey1
Accepted: 10 November 2023 ¬© The Author(s), under exclusive licence to Springer Nature B.V. 2023
Abstract For the past 40 years, philosophers have generally assumed that a key to understanding mental representation is to develop a naturalistic theory of representational content. This has led to an outlook where the importance of content has been heavily inflated, while the significance of the representational vehicles has been somewhat downplayed. However, the success of this enterprise has been thwarted by a number of mysterious and allegedly non-naturalizable, irreducible dimensions of representational content. The challenge of addressing these difficulties has come to be known as the ‚Äúhard problem of content‚Äù (Hutto & Myin, 2012),


For example, various accounts develop the idea that representational structures in the brain stand in some sort of structural similarity relation to their target, thereby operating as maps and models (e.g., Cummins 1989; Ramsey 2007; Gladziejewski and Milkowski 2017). And yet these philosophical accounts focus almost exclusively upon issues associated with the possession of content, and rarely attempt to answer questions about just how, exactly, neurological states and structures actually are structurally similar to some environmental target.


## Double Descent


Discovery of the Double Descent Effect
Timeline of Discovery
Early Observations (1989): The phenomenon that would later be known as "double descent" was first observed in specific machine learning models as early as 1989. However, these were isolated findings and not broadly recognized or named as a distinct effect within the machine learning community at the time.

Formal Recognition (2019): The term "double descent" and its formal broad conceptualization were introduced by Mikhail Belkin and collaborators in 2019. Their work synthesized previous scattered observations and systematically demonstrated the phenomenon across a range of models, including neural networks and classical machine learning techniques.

How Double Descent Was Discovered
Contradictory Empirical Trends: Researchers noted that, contrary to classical statistical theory (which suggested that increasing model complexity would inevitably lead to overfitting and worse test performance), modern practitioners found that making models larger‚Äîeven beyond the point where they could interpolate (fit) the training data‚Äîoften led to improved test performance.

Challenge to the Bias-Variance Tradeoff: Traditionally, test error was expected to follow a U-shaped curve as model complexity increased‚Äîdecreasing initially (bias reduction), then increasing due to overfitting (variance increase). Belkin et al. demonstrated that after this initial peak in test error, further increases in complexity often caused the test error to drop again, forming the characteristic "double descent" curve.








## 2025-07-18 00:27:42

Based on my search results, I can provide you with a comprehensive answer about the relationship between information theory "typicality" (particularly the Asymptotic Equipartition Property) and modal logic semantics.

## Applications of Information Theory Typicality to Modal Logic Semantics

After conducting extensive research, I found that **direct applications of information theory typicality or the Asymptotic Equipartition Property (AEP) to modal logic semantics are extremely limited and largely unexplored**. However, there are several interesting connections and related research areas that are worth discussing:

### **What is the AEP and Typicality?**

The Asymptotic Equipartition Property is a fundamental theorem in information theory that states that for independent identically distributed (i.i.d.) random sequences, the probability converges to the entropy $$ H(X) $$ as the sequence length approaches infinity[1][2][3]. The AEP leads to the concept of **typical sets** - sequences that have probability close to $$ 2^{-nH(X)} $$ and collectively account for almost all probability mass despite being an exponentially small fraction of all possible sequences[2][3].

### **Current State of Research**

While there are no direct applications of the AEP to modal logic semantics, several research directions explore related concepts:

#### **1. Modal Logic and Information Theory Connections**

The search revealed several works that connect modal logic with information-theoretic concepts:

- **Modal Logic of Information**: Researchers have developed modal logics specifically designed to reason about information, such as Misiuna's work on "A Modal Logic of Information" which uses modal structure K4 to avoid inconsistencies with intuitive concepts of information[4][5].

- **Epistemic Modal Logic with Probability**: There's substantial work on probabilistic modal logic, where modal operators are combined with probability measures over possible worlds[6][7][8][9].

#### **2. Typicality in Modal Logic (Different Concept)**

Importantly, there's a separate line of research on **typicality in modal logic** that is unrelated to information theory. This involves:

- **Propositional Typicality Logic (PTL)**: This logic introduces a typicality operator to reason about "typical" or "normal" situations, but this concept of typicality is semantic rather than information-theoretic[10][11][12][13].

- **Description Logics with Typicality**: Extensions of description logics that can represent typical properties and defeasible inheritance[14][15].

#### **3. Quantum Extensions**

The most developed connection between information theory and logic appears in the quantum domain:

- **Quantum Logical Entropy**: David Ellerman and others have developed quantum extensions of logical entropy that provide alternatives to Shannon entropy in quantum information theory[16][17][18][19].

- **Quantum Modal Logic**: Some research explores modal logic in quantum contexts, though not specifically connecting to typicality[20].

### **Potential Research Directions**

While direct applications are lacking, several promising research directions could emerge:

#### **1. Modal Logic Semantics for Information Sources**
The AEP could potentially be applied to modal logic by:
- Viewing possible worlds as outputs of a stochastic source
- Using typicality to define "normal" or "expected" worlds in modal semantics
- Developing modal operators that distinguish between typical and atypical worlds

#### **2. Epistemic Logic with Bounded Rationality**
Information-theoretic typicality could inform epistemic logic by:
- Modeling agents with limited computational resources who focus on typical sequences
- Using entropy to measure the information content of epistemic states
- Developing modal operators for "informationally efficient" knowledge

#### **3. Dynamic Modal Logic with Information Flow**
The AEP could be relevant for dynamic epistemic logic by:
- Modeling how information updates affect the typicality of possible worlds
- Using entropy rates to measure information flow between agents
- Developing modal operators for "information-preserving" updates

### **Conclusion**

Currently, there are **no established applications** of information theory typicality or the AEP to modal logic semantics. The existing research on "typicality" in modal logic refers to a completely different concept related to default reasoning and non-monotonic logic. 

However, this represents a significant **research opportunity**. The mathematical tools from information theory, particularly the AEP and typical sets, could provide new insights into modal logic semantics, especially in epistemic contexts where information processing and bounded rationality are relevant. Such work would likely require developing new theoretical frameworks that bridge information theory and modal logic in novel ways.

The absence of existing work in this area suggests that your question identifies a potentially fruitful but unexplored research direction at the intersection of information theory and modal logic.

[1] https://blog.csdn.net/qq_39599295/article/details/113927010
[2] https://poincare.matf.bg.ac.rs/nastavno/viktor/The_Asymptotic_Equipartition_Property.pdf
[3] https://www.wikiwand.com/en/articles/Asymptotic_equipartition_property
[4] https://apcz.umk.pl/LLP/article/download/LLP.2012.003/741/2961
[5] https://apcz.umk.pl/LLP/article/view/LLP.2012.003
[6] https://arxiv.org/html/2404.15461v1
[7] https://cdn.aaai.org/AAAI/2007/AAAI07-077.pdf
[8] https://arxiv.org/pdf/2404.15461.pdf
[9] https://semanticsarchive.net/Archive/ThhZjI5Z/Probabilistic%20semantics%20for%20epistemic%20modals%20LP%20final%20-%20Guillermo%20Del%20Pinal.pdf
[10] https://www.cair.org.za/sites/default/files/2019-08/MadeiraBookChapter.pdf
[11] https://academic.oup.com/jigpal/advance-article/doi/10.1093/jigpal/jzae086/7713733
[12] https://www.sciencedirect.com/science/article/pii/S000437021830506X
[13] https://orca.cardiff.ac.uk/id/eprint/125890/1/2019_AIJ_PTL.pdf
[14] https://arxiv.org/abs/1811.02366
[15] https://philarchive.org/archive/LIEADL
[16] https://pmc.ncbi.nlm.nih.gov/articles/PMC7513204/
[17] https://philsci-archive.pitt.edu/24959/1/A_new_logical_measure_QInfo4.pdf
[18] https://philsci-archive.pitt.edu/15013/1/_entropy-20-00679--reprint.pdf
[19] https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2770162
[20] https://en.wikipedia.org/wiki/Quantum_logic
[21] https://pages.jh.edu/rrynasi1/FormalMethods/2017Fall/Slides/5.1_Modal_Logic.pdf
[22] https://en.wikipedia.org/wiki/Modal_logic
[23] https://arxiv.org/abs/2212.14700
[24] https://tsinghualogic.net/JRC/wp-content/uploads/2021/07/5thLecturewithReference.pdf
[25] https://academic.oup.com/book/50190/chapter/422031224
[26] http://arxiv.org/pdf/2212.14700v1.pdf
[27] https://www3.cs.stonybrook.edu/~cse541/Spring2009/cse541BasicModalLogic.pdf
[28] https://plato.stanford.edu/entries/logic-modal/
[29] https://www.illc.uva.nl/Publications/ResearchReports/PP-2006-30.text.pdf
[30] https://aclanthology.org/P92-1045.pdf
[31] https://h-joudeh.github.io/IT_5XSE0_Notes_Ch3.pdf
[32] https://www.mimuw.edu.pl/~nguyen/mpl.pdf
[33] https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=5b56d60560df4957d8d049fb586cbd43bac1dcf3
[34] https://web.stanford.edu/class/ee376a/files/2017-18/lecture_4.pdf
[35] https://projecteuclid.org/journals/notre-dame-journal-of-formal-logic/volume-14/issue-2/A-simplified-semantics-for-modal-logic/10.1305/ndjfl/1093890890.pdf
[36] https://pmc.ncbi.nlm.nih.gov/articles/PMC3099461/
[37] http://www.aiml.net/volumes/volume9/Fernandez-Duque-Joosten.pdf
[38] https://www.math.ucla.edu/~joan/logicalKripkelookhandout.pdf
[39] https://paolosantorio.net/KS-NASSLLI2018.pdf
[40] https://en.wikipedia.org/wiki/Kripke_structure
[41] http://therisingsea.org/notes/talk-shawn-kripke.pdf
[42] https://www.wikiwand.com/en/articles/Kripke_structure_(model_checking)
[43] https://escholarship.org/uc/item/6df2g3xv
[44] https://www.uni.lodz.pl/fileadmin/Projekty/EXTENDD/BSL/3__1/03_1_1.pdf
[45] https://iep.utm.edu/modal-lo/
[46] https://en.wikipedia.org/wiki/Kripke_semantics
[47] https://arxiv.org/pdf/1310.6439.pdf
[48] https://staff.fnwi.uva.nl/j.vanbenthem/hml-blackburnvanbenthem.pdf
[49] https://www.cs.virginia.edu/~rmw7my/papers/mml.pdf
[50] https://ui.adsabs.harvard.edu/abs/2013arXiv1310.6439P/abstract
[51] https://www.jstor.org/stable/27823331
[52] http://isaacwilhelm.com/pdfs/typ.pdf
[53] https://arxiv.org/pdf/1903.12518.pdf
[54] https://golem.ph.utexas.edu/category/2011/04/category_theoretic_modal_logic.html
[55] https://www.logic.at/staffpages/revantha/Tutorial-lectures-1-3.pdf
[56] https://www.sciencedirect.com/science/article/pii/S1570246407800085
[57] https://arxiv.org/pdf/2209.12564.pdf
[58] http://marilaur.info/pws.htm
[59] https://arxiv.org/html/2406.02108v1
[60] https://philarchive.org/archive/MENMST
[61] https://projecteuclid.org/journals/notre-dame-journal-of-formal-logic/volume-38/issue-4/Information-and-Impossibilities/10.1305/ndjfl/1039540766.pdf
[62] https://homepages.tuni.fi/antti.kuusisto/hamburg.pdf
[63] http://people.umass.edu/scable/LING620-SP18/Handouts/9.Modals1.pdf
[64] http://www.cs.ox.ac.uk/people/michael.wooldridge/pubs/ker95/subsection3_2_3.html
[65] https://philosophy.stackexchange.com/questions/89965/what-is-the-philosophers-take-on-information-and-thermodynamic-entropy
[66] https://www2.mathematik.tu-darmstadt.de/~otto/papers/mlhb.pdf
[67] https://philarchive.org/archive/SOCPWS
[68] https://philsci-archive.pitt.edu/13213/1/Logic-to-information-theory3.pdf
[69] https://bpb-us-e1.wpmucdn.com/blogs.gwu.edu/dist/d/5847/files/2024/06/ECC_Mod_03_FormalApproaches.pdf
[70] https://www.informationphilosopher.com/knowledge/possible_worlds.html
[71] https://philsci-archive.pitt.edu/10230/1/IntroLogicalEntropy-std1.pdf
[72] http://people.umass.edu/scable/LING620-SP22/Handouts/15.Modals3.pdf
[73] https://plato.stanford.edu/entries/possible-worlds/
[74] https://drops.dagstuhl.de/storage/00lipics/lipics-vol326-csl2025/LIPIcs.CSL.2025.17/LIPIcs.CSL.2025.17.pdf
[75] https://iep.utm.edu/ep-moda/
[76] https://quod.lib.umich.edu/cgi/p/pod/dod-idx/probability-for-epistemic-modalities.pdf?c=phimp%3Bidno%3D3521354.0021.033%3Bformat%3Dpdf
[77] https://philosophy.stackexchange.com/questions/7902/understanding-information-states-in-epistemic-modal-logics
[78] http://www.tark.org/proceedings/tark_jul22_98/p175-heifetz.pdf
[79] https://arxiv.org/pdf/2411.15940.pdf
[80] http://www.aiml.net/volumes/volume10/Eijck-Schwarzentruber.pdf
[81] https://pdfs.semanticscholar.org/a4c6/9fe3278045c86ec50dfb76eeb69706a66174.pdf
[82] https://philpapers.org/archive/GOLPFE-2.pdf
[83] https://arxiv.org/pdf/1303.5393.pdf
[84] https://arxiv.org/pdf/2105.10164.pdf
[85] https://bd.openlogicproject.org/bd-screen.pdf
[86] https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3077049
[87] https://is.buaa.edu.cn/__local/5/99/51/C69730CF8F0A7AC72DD7AE451CE_CC6A5599_12BD4E.pdf
[88] https://brilliant.org/wiki/entropy-information-theory/
[89] https://plato.stanford.edu/entries/dynamic-epistemic/appendix-D-normal-ML.html
[90] https://web.stanford.edu/class/ee276/files/previous_notes/lecture_3.pdf
[91] https://en.wikipedia.org/wiki/Entropy_(information_theory)
[92] https://cgi.csc.liv.ac.uk/~wiebe/pubs/Documents/ARTINT2636.pdf
[93] https://arxiv.org/html/2501.00612v1
[94] https://www.informatik.uni-bremen.de/tdki/lehre/ws11/logic/mat/9_modal_4on1.pdf
[95] https://www.numberanalytics.com/blog/asymptotic-equipartition-property-information-theory

## 2025-07-18 17:02:07

KL divergence is not symmetric because it quantifies how well one distribution (Q) can approximate or "explain" another (P). If Q assigns zero probability to something P considers possible, it‚Äôs a fatal error in approximation. But not the other way around.

## 2025-07-18 17:04:06

Yes, this is consistent with:

Gibbs‚Äô inequality, which says:

‚àë
ùëñ
ùëù
ùëñ
log
‚Å°
ùëù
ùëñ
ùëû
ùëñ
‚â•
0
i
‚àë
‚Äã
 p 
i
‚Äã
 log 
q 
i
‚Äã
 
p 
i
‚Äã
 
‚Äã
 ‚â•0
with equality iff 
ùëù
=
ùëû
p=q

And the definition of KL divergence:

ùê∑
(
ùëÉ
‚Äñ
ùëÑ
)
=
‚àë
ùë•
ùëÉ
(
ùë•
)
log
‚Å°
ùëÉ
(
ùë•
)
ùëÑ
(
ùë•
)
D(P‚ÄñQ)= 
x
‚àë
‚Äã
 P(x)log 
Q(x)
P(x)
‚Äã
 
(noted explicitly in the "Golden Lemma" and the entropy sections of the PDF)

## 2025-07-19 11:58:47

Information Theory as a Bridge Between Language Function and Language Form.pdf
Source guide
Summary
This academic article proposes information theory as a bridge between formal and functional theories of language, arguing that language forms arise from optimizing communication under various constraints. The authors introduce core information-theoretic concepts like entropy, conditional entropy, and mutual information to quantify communication and complexity in a theory-neutral way, emphasizing that information content depends on probability, not representation. They demonstrate how established linguistic formalisms, such as MaxEnt grammars, can be derived by minimizing specific information-theoretic objective functions. A key case study focuses on locality properties in natural language, explaining how the tendency for related linguistic elements to appear close together stems from an efficient trade-off between memory usage and surprisal during language processing. The article ultimately suggests that human language optimizes information transfer while minimizing a defined complexity, offering a unified, formal, and functional theory of language.

## 2025-07-19 12:00:12

Information Theory as a Bridge Between Language Function and Language Form

Richard Futrell
Richard Futrell1*Michael HahnMichael Hahn2
1Department of Language Science, University of California, Irvine, Irvine, CA, United States
2Department of Linguistics, Stanford University, Stanford, CA, United States
Formal and functional theories of language seem disparate, because formal theories answer the question of what a language is, while functional theories answer the question of what functions it serves. We argue that information theory provides a bridge between these two approaches, via a principle of minimization of complexity under constraints. Synthesizing recent work, we show how information-theoretic characterizations of functional complexity lead directly to mathematical descriptions of the forms of possible languages, in terms of solutions to constrained optimization problems. We show how certain linguistic descriptive formalisms can be recovered as solutions to such problems. Furthermore, we argue that information theory lets us define complexity in a way which has minimal dependence on the choice of theory or descriptive formalism. We illustrate this principle using recently-obtained results on universals of word and morpheme order.


## Bickhard mechanism is not enough 

critique of mechanistic explanation frameworks for mental representations, largely by arguing against a foundational substance or particle metaphysics and by critiquing encodingist models of representation which typically underpin such frameworks.
Here are the main arguments against mechanism as an explanation framework for mental representations:
1. Problems with Substance/Particle Metaphysics (Underlying Mechanism) The "interactivist model" argues for a shift from a metaphysical framework of substance to one of process, as substance assumptions fundamentally block genuine ontological emergence, especially the emergence of normativity.
‚Ä¢ Conceptual Impossibility of Emergence: The heritage of the Greeks includes fundamentally aporetic metaphysical positions, such as the substance/particle framework, which render phenomena like the emergence of normativity conceptually impossible. This framework, derived from Parmenides and elaborated by Empedocles, Democritus, and Aristotle, posits unchanging basic substances or particles as the world's constituents.
‚Ä¢ Split between Realms: The acceptance of an unchanging metaphysical ground (substance or particle) strongly leads to a split between the realm of substances/particles (characterized by cause and fact) and the realm of mental phenomena (intentionality, normativity, modality). This split has persisted for over two millennia and is not easily overcome within such a framework.
‚Ä¢ Hume's Argument Against Normative Emergence: Hume's argument, that nothing new can be obtained in the conclusion of a valid argument beyond configurations of what is already available in the premises, precludes any kind of emergence and is a manifestation of the restriction to combinatorics inherent in the basic substance/particle framework. However, this argument is considered unsound because it falsely assumes all definition permits back-translation, ignoring implicit definition.
‚Ä¢ Kim's Pre-emption Argument Against Causal Efficacy of Higher Levels: Jaegwon Kim's argument states that all causality resides in the lowest level of physical entities, making any new causal properties in higher-level organization merely "causal regularities". This argument implies that organization cannot have causal power beyond that of its constituent particles, thereby begging the question against emergence by assuming a particle framework that excludes organization as a locus for causal power.
‚Ä¢ Inconsistency with Contemporary Physics: The fundamental assumption that a particle metaphysics is adequate is false. Contemporary physics, particularly quantum field theory, demonstrates that there are no particles at all; instead, the fundamental constituents of the world are dynamic quantum fields in a dynamic space-time, which are inherently processes. The particle framework has "run its course".
‚Ä¢ Explanatory Default: In a substance framework, the explanatory default is inertness or lack of change, meaning change requires explanation. In a process framework, change becomes the explanatory default, and stability requires explanation.
2. Problems with Encodingist Models of Representation (Common in Mechanistic Views of Mind) Many representational models, often linked to mechanistic views, assume that representation is constituted in some special encoding relationship (causal, nomological, informational) between the representation and the represented. The interactivist model argues that encodingism is logically incoherent and impossible for foundational representations.
‚Ä¢ Necessarily Derivative: Encodings are stand-ins for other representations; they change the form of representations but borrow content from elsewhere. They cannot provide non-derivative representations of the environment because epistemic agents must already have representations of their worlds for encodings to be derived.
‚Ä¢ "Too Many Correspondences": For any presumed encoding correspondence (e.g., between brain activity and a table), there are innumerable other causal relationships (e.g., with the retina, light patterns, quantum activities, historical manufacturing). Encodingism struggles to explain which one of these is the "representational instance" without appealing to an external observer, which leads to circularity.
‚Ä¢ Piaget's Copy Argument: Representations cannot be copies of the world because an agent would have to already know the world to construct such copies, leading to circularity. This points to the failure of these models to capture representational normativity without presupposing it.
‚Ä¢ Failure to Account for Emergence: Representation did not exist universally and must have emerged. Models that cannot account for this emergence are incomplete or refuted. Fodor's argument for the innateness of a base set of atomic representations, due to the inability of learning models to generate new (emergent) representations, highlights this gap.
‚Ä¢ Lack of System Detectable Error: A critical criterion for a successful model of representation is the possibility for an epistemic agent to detect error in its own representations. If content is externally related and inaccessible to the organism, system detectable error is impossible, which, in turn, makes error-guided behavior and learning impossible. The "radical skeptical argument" demonstrates this challenge: one cannot step outside oneself to compare representation with reality.
‚Ä¢ Externally Related Content Leads to Infinite Regress: If representational content is externally related to the representation (i.e., not necessary for the representation to be what it is), then it must be added or interpreted. This leads to an infinite regress of interpreters or undischarged homunculi circularly providing the content.
‚Ä¢ Conflation of Contact and Content: Standard mechanistic/encodingist models of perception conflate "differentiations" (a necessary form of contact with the environment) with "representations". In the interactive model, differentiations are just that‚Äîdifferentiations‚Äîand do not themselves constitute representations or bear truth values; they serve as the functional basis for setting up anticipatory content, which is representational.
‚Ä¢ Epiphenomenality (in Etiological Models): Models like Millikan's and Dretske's, which base function and representation on etiological (evolutionary or learning) history, render function and representation epiphenomenal. This means that whether something has a "function" (based on its history) makes no difference to its current causal or dynamic properties. The "science-fiction lion" example illustrates this: a dynamically identical lion with no history would have no functions according to these models.
‚Ä¢ Circular Explanations: Etiological models are circular in deriving normativity because the "usefulness" that must be selected for early in evolutionary history is itself already normative, presupposing the very normativity they aim to explain.
‚Ä¢ Context Dependency Ignored or Underestimated: Encodingist approaches often treat words as "Names" that encode singular meanings, implying context-independence. However, language is profoundly context-dependent, and such a "limiting case" is unreachable and flawed.
‚Ä¢ Misconception of Utterances: Encodingism typically treats utterances as directly representational. In contrast, the interactive model views utterances as interactions with social realities (situation conventions) and representations within them, not as representations in themselves.
In conclusion, the sources argue that mechanistic explanation frameworks, rooted in substance/particle metaphysics and reliant on encodingist models, are conceptually and empirically bankrupt, failing to account for emergence, normativity, error detection, internal content, and the actual dynamics of cognitive and linguistic phenomena. They propose a process-based and interactive model as a more viable alternative.


## 2025-07-19 19:32:17
Dietrich's 
The Architecture of Mental Representation
üéâ New! Share publicly
Sources
Chat
Studio

The Structure of Thought: Compositionality in Cognition
(Saved responses are view only)
Compositionality, in the context of representations, refers to the property where representations have a compositional, or role-argument, structure that facilitates combination. This means that more complex concepts can be formed from more primitive ones, similar to how morphological units representing objects combine with units representing relations in language.
Here's a breakdown of compositionality as discussed in the sources:
‚Ä¢ Definition and Core Idea:
    ‚ó¶ The assumption is that representations have a compositional, or role-argument, structure. This allows concepts to combine, which is particularly evident in language, where the juxtaposition of units for objects (nouns) and relations (verbs) describes actions.
    ‚ó¶ Cognitive processes that rely on a role-argument structure require mechanisms sensitive to the bindings between predicates and their arguments. Such structure-sensitive processes are often more complex than those operating on non-compositional structures.
‚Ä¢ Evidence and Arguments for Compositionality:
    ‚ó¶ Analogy and Comparison: Models of comparison, such as Gentner's structure-mapping theory, propose that comparisons seek structurally consistent matches that obey parallel connectivity (arguments to matching predicates must also match) and one-to-one mapping (each element in one structure matches at most one in another). This process inherently considers bindings between predicates and their arguments.
    ‚ó¶ Visual Object Recognition: Research suggests that visual representations are characterized by components that encode spatial relations between parts. For example, Biederman's work shows that object recognition is disrupted more by eliminating information at junctions (which carries relational information) than by removing an equivalent amount of line information between joints. This implies a need for relational elements in visual arrays and supports the idea of representations with role-argument structures.
    ‚ó¶ Linguistic Competence: Compositional structure appears necessary for linguistic competence, enabling people to distinguish between sentences like 'The Giants beat the Jets' and 'The Jets beat the Giants' based on the roles of the arguments. The ability to track others' beliefs (propositional attitudes) also requires encoding elements in the original proposition and meta-propositions, suggesting a need for representations with a role-argument structure.
    ‚ó¶ Categorization and Discrimination: While not explicitly stated as "compositional," the source implies a connection. The ability to categorize inputs, which involves imposing classes of sameness, and to discriminate inputs (noticing multiple things in the input stream), suggests a need for discrete representations. The "parts" of a representation become accessible when discrete representations are present, which is crucial for detailed comparisons and finding antonyms, a function that high-dimensional semantic spaces without discrete representations struggle with. The ability to operate on objects with part identity rather than overall identity also requires discrete representations. These points align with the need to distinguish and relate parts of information, a characteristic often associated with compositional structures.
‚Ä¢ Arguments Against and Limitations:
    ‚ó¶ Complexity: Constructing and processing compositional structures can require significant effort, which some argue goes beyond what is needed for many cognitive tasks.
    ‚ó¶ Cognitive Development: Critics, like Thelen and Smith, argue that positing complex representations and processes like compositional structures for infant development (e.g., object permanence) seems overly complex given observed developmental patterns. They suggest infants may react to regularities detected by the visual system, forming expected trajectories in a continuous state space.
    ‚ó¶ Dynamic Systems Perspective: Noncompositional, continuous, and transient representations are particularly good at explaining the dynamical aspects of sensorimotor interaction.
‚Ä¢ Reconciliation and Conclusion:
    ‚ó¶ Despite arguments against it, the source strongly suggests that even dynamic systems accounts, when thoroughly examined, implicitly require complex compositional structures to explain phenomena like infant object permanence. The explanation requires an account of how trajectory divergences distinguish between what matters and what does not, which points to a recognition of spatial relationships and the need for storing discrete, compositional spatial relations.
    ‚ó¶ In sum, while noncompositional representations are likely used by low-level perceptual systems and for coordinating bodily movements, compositional structures are necessary for higher-level cognitive and linguistic processing.
    ‚ó¶ It is acknowledged that different kinds of representations are required depending on the mental or cognitive process in focus, and that no single type of representation can explain everything. However, the ability to recognize novel instances in the world and engage in complex thought necessitates certain levels of compositional structure

## 2025-07-21 15:24:42

Black-Box View of a Channel: In Shannon‚Äôs framework, a communication channel is treated abstractly as a system (a ‚Äúblack box‚Äù) that transfers symbols from sender to receiver. The internal mechanism of the channel can be ignored in the model; what matters is how inputs map to outputs statistically. Shannon‚Äôs famous schematic (Figure¬†1 of his paper) shows an information source feeding a transmitter, which sends signals through a channel, possibly corrupted by a ‚Äúnoise source,‚Äù to produce outputs at the receiver
itsoc.org
. From the outside, both noiseless and noisy channels can be characterized by their input-output behavior (the set of possible outputs for each input, and their probabilities) without needing to know the physical details of the channel‚Äôs operation.

## 2025-07-21 15:25:12

Noiseless Channel as a Black Box: A noiseless channel can be modeled as a deterministic black box ‚Äì effectively an identity transformation (or a fixed invertible function) on the input symbols. Given an input symbol, the output is always the same symbol (or a uniquely corresponding symbol) with certainty
itsoc.org
. In black-box terms, the channel‚Äôs transition probability matrix has entries that are 1 for the intended input‚Üíoutput mapping and 0 for any other outcome. Because of this one-to-one behavior, an external observer sees a reliable, predictable mapping. The functional abstraction of a noiseless channel is simply a conduit that faithfully reproduces the input at the output (up to any fixed encoding/decoding transformation).
Noisy Channel as a Black Box: A noisy channel is modeled as a black box with a probabilistic mapping from inputs to outputs. Shannon represents a discrete memoryless noisy channel by a set of transition probabilities $p_i(j)$ ‚Äì the probability that input symbol $i$ is received as symbol $j$
itsoc.org
. (More generally, if the channel has internal states, one can specify state-dependent transition probabilities
itsoc.org
, but the idea is the same: the channel is a stochastic system.) Thus, from an external view, a noisy channel is characterized by a conditional probability distribution $P(\text{Output}=j \mid \text{Input}=i)$ for all symbol pairs. One does not need to know exactly why or how errors occur inside; the channel can be treated as a random-function black box. For example, an observer might know that when a ‚Äú0‚Äù is sent, there is a 1% chance the channel outputs ‚Äú1‚Äù instead ‚Äì without needing to know the physical cause of the flip. Functionally, this is an abstraction where the channel adds some uncertainty between input and output.
Unified Treatment: Both types of channels can be analyzed with the same black-box tools ‚Äì mainly, probability and information measures. In fact, Shannon‚Äôs noisy-channel model is explicitly built as a generalization of the noise-free channel model
itsoc.org
. We can think of the noiseless channel‚Äôs black box as a special case of the noisy channel‚Äôs black box where the transition probabilities are degenerate (100% probability on the correct output). In analysis, one simply plugs in the appropriate transition probabilities and computes measures like entropy, mutual information, and capacity. This functional abstraction allows engineers and theorists to ignore hardware specifics and focus on the information behavior: the noiseless channel has an uncertainty-free mapping, and the noisy channel has an uncertainty-augmented mapping.

## 2025-07-21 15:51:10

Black-Box Similarity in Analysis: When performing a black-box analysis, we focus on external performance measures (like error rates, mutual information, capacity), not the internal cause of errors. From this viewpoint, a noiseless channel simply has zero error probability for each symbol, and a noisy channel has some error probability model ‚Äì a difference in degree rather than kind. Both can be described by a matrix of probabilities or a transition rule.

## 2025-07-21 15:51:25

Black-Box Similarity in Analysis: When performing a black-box analysis, we focus on external performance measures (like error rates, mutual information, capacity), not the internal cause of errors. From this viewpoint, a noiseless channel simply has zero error probability for each symbol, and a noisy channel has some error probability model ‚Äì a difference in degree rather than kind. Both can be described by a matrix of probabilities or a transition rule.



## 2025-07-26 18:31:44

Investigating the concept of representation in the neural and psychological sciences Luis H. Favela1,2 and Edouard Machery3,4,5

https://doi.org/10.3389/fpsyg.2023.1165622

The concept of representation is commonly treated as indispensable to research
on brains, behavior, and cognition. Nevertheless, systematic evidence about the
ways the concept is applied remains scarce. We present the results of an experiment
aimed at elucidating what researchers mean by ‚Äúrepresentation.‚Äù Participants
were an international group of psychologists, neuroscientists, and philosophers
(N‚Äâ=‚Äâ736). Applying elicitation methodology, participants responded to a survey with
experimental scenarios aimed at invoking applications of ‚Äúrepresentation‚Äù and five
other ways of describing how the brain responds to stimuli. While we find little
disciplinary variation in the application of ‚Äúrepresentation‚Äù and other expressions
(e.g., ‚Äúabout‚Äù and ‚Äúcarry information‚Äù), the results suggest that researchers exhibit
uncertainty about what sorts of brain activity involve representations or not;
they also prefer non-representational, causal characterizations of the brain‚Äôs
response to stimuli. Potential consequences of these findings are explored, such
as reforming or eliminating the concept of representation from use.

## 2025-07-26 18:32:12

Mental vs. Neural Representation
John Krakauer
Asserts that mental representations are real and undeniable, evidenced by "representation-rich behavior" and their selective loss in diseases like Alzheimer's. Argues that the use of "representation" by neuroscientists (for "informational content" or correlation) has "absolutely nothing to do with mental representation". The problem arises when the word "representation" is allowed to "overlap" between these distinct uses, creating the false impression of an explanation for mental representation at the neural level. Advocates for not using the word "representation" for mere "informational content" in neuroscience, as it won't explain rich mental representation. States that structural neural representations (isomorphism) are also a "non-starter". Concludes that we don't know the neural story for mental representation, and "prematurely using the word for neural data" is unhelpful.

## 2025-07-26 18:38:32

The discussion explores the multifaceted and often debated term "representation" within cognitive science and neuroscience, examining its various uses, the clarity of its application, and its implications for understanding mental capacities.

Here is a table summarizing the arguments made by each discussant at different stages of the conversation:

| Stage/Subject of Discussion | Discussant | Arguments / Key Points |
| :-------------------------- | :--------- | :--------------------- |
| **Introduction & Problem Definition** | Paul Middlebrooks | Introduces the core issue: **confusion and unclarity in the use of "representation"** in cognitive sciences and neurosciences. Highlights that Luis Favela and Eduard Machery's survey aimed to address this, and their conclusion was that the term is indeed used in a "confused and unclear way". Mentions follow-up arguments and responses from other guests. |
| | Luis Favela | Expressed **discontent with philosophers assuming neuroscientists have a clear, well-thought-out sense of "representation"**. Questioned whether neuroscientists themselves understand the term. Initiated an **empirical project (a survey)** to gather systematic evidence on how the term is actually used. |
| | Eduard Machery | Collaborated with Luis Favela on the survey to investigate the usage of "representation" in practice. |
| **Survey Methodology & Driving Questions** | Luis Favela | Sent a survey to thousands of people (philosophers, cognitive scientists, psychologists, neuroscientists), receiving ~800 responses. The survey included questions about cognition involving representation and vignettes depicting neuroscience research, asking how confidently respondents would use terms like "about," "conveying information," or "representing" neural activity in relation to a stimulus. |
| | Eduard Machery | Highlighted three key questions driving the project: 1. **Scale**: At what level are representations found (single neurons, small groups, brain areas, distributed networks)? 2. **Causal Relation**: Must the relation between stimulus and brain activation have specific properties (e.g., reliability) to count as a representation? 3. **Functionality**: Does the network need to be embedded in a broader network to be considered functional/used by the rest of the brain?. |
| **Survey Findings: Unclear & Confused** | Luis Favela | Concluded that the concept of "representation" is **"unclear and confused"** in its application across various sciences. Interpreted a **"hesitancy"** (middle scores on Likert scale) to strongly describe neural activity as "about" a stimulus as indicative of a lack of clarity. |
| | Eduard Machery | Defined **"lack of clarity"** as users having few commitments about what follows from using the word "representation" (e.g., about scale, causal relation, or function). Defined **"confusion"** based on neuroscientists' **unwillingness to say the brain misrepresents**. Argued that neuroscientists **confuse "representations" (which can misrepresent, like a map) with "natural signs" (which indicate but cannot misrepresent, like smoke)**. |
| **Proposed Ways Forward: Reform, Eliminate, Understand** | Luis Favela & Eduard Machery | Suggested three paths: **reform** the concept (make it precise), **eliminate** the term, or **understand why** the imprecise/confused notion is nonetheless functional or useful. |
| | Eduard Machery | **Reforming** involves specifying commitments or drawing distinctions (e.g., sign vs. representation). **Elimination** is a view John Krakauer is sympathetic to. **Understanding** involves recognizing that lack of clarity or confusion can be **functional for science**, similar to the concept of "gene" in biology, aiding cross-disciplinary communication. Became **skeptical of top-down reform** efforts due to semantic drift, noting historical examples like "temperature" took hundreds of years for bottom-up reform. |
| | Luis Favela | Supported the idea that **imprecision can be a virtue** for communication across different biological fields (e.g., molecular vs. behavioral biologists talking about "gene"). Expressed a **stronger view for eliminating the term "representation"** if it doesn't refer to anything real or is too conceptually confused. |
| **Initial Responses & Core Concerns** | Paul Middlebrooks (Summarizing) | Summarizes John Krakauer's view: dislikes accepting various uses, wants to keep "mental representation" alive and reform it. Summarizes Rosa Cao's view: doesn't accept survey results, believes confident usage in various ways is possible. Summarizes Francis Egan's view: okay with "representation" as a "causal thin gloss". |
| | John Krakauer | Finds the discussion "startlingly irritating" due to a **"failure to think hard about the phenomenology"** of mental capacities. |
| **Mental vs. Neural Representation** | John Krakauer | Asserts that **mental representations are real and undeniable**, evidenced by "representation-rich behavior" and their selective loss in diseases like Alzheimer's. Argues that the use of "representation" by neuroscientists (for "informational content" or correlation) has **"absolutely nothing to do with mental representation"**. The problem arises when the word "representation" is allowed to **"overlap"** between these distinct uses, creating the false impression of an explanation for mental representation at the neural level. **Advocates for *not* using the word "representation" for mere "informational content" in neuroscience**, as it won't explain rich mental representation. States that structural neural representations (isomorphism) are also a "non-starter". Concludes that **we don't know the neural story for mental representation**, and "prematurely using the word for neural data" is unhelpful. |
| | Rosa Cao | **Agrees with most of John's points but disagrees with eliminating the term in neuroscience**. Argues that neuroscientists (especially computational neuroscientists) **should be allowed to use "representation" in diverse ways** for things not related to mental representations, provided they **avoid "equivocating"** and pretending an explanation exists when it doesn't. Believes that, in principle, a **diversity of uses is fine**. |
| **The Impact of Confusion** | John Krakauer | Contends that the podcast itself is evidence that having the same word for "utterly different meanings is hugely confusing," leading to confusion among students and general public. |
| | Rosa Cao | Questions **"what does it matter" if people are confused**, as long as they use the terms in ways they understand. |
| | Eduard Machery | Argues that confusion **matters because it impacts philosophers' conclusions** about progress in understanding mental capacities. Also, it **influences the *kind of explanations* neuroscientists pursue**, potentially pushing them towards specific models of neural processing or relation to psychology. |
| **Frankie's Diagnosis: Pragmatic Motivation for the Term** | Francis Egan (Frankie) | Agrees that neuroscientists' use of "representation" is often purely "information theoretic co correlation causal". Her diagnosis for *why* they persist in using representational talk is that it's **"always pragmatically motivated"**. Characterizing something as representational allows for its **evaluation for accuracy** (true vs. misrepresentation), which is beneficial even if neuroscientists don't primarily care about semantic accuracy. This talk provides **"connective tissue"** between neuroscientific "causal mechanical accounts" and the pre-theoretically characterized, normative/intentional **cognitive capacities** they aim to explain, helping to **justify research proposals**. Neuroscience has an **"intelligibility constraint"** that other physiologies lack, requiring outputs to be seen as rational, thus motivating representational talk. |
| **Debate: Assessment & Neuroscientists' Understanding** | Eduard Machery | Challenges Frankie, stating that neuroscience *does* care about assessment, citing **optimality models like Bayesian models** that assume optimal representation. Notes that neuroscientists **distinguish between causal relations and intentional vocabulary** like "representing," showing ambivalence towards the latter. |
| | Francis Egan (Frankie) | Clarifies that the assessment in neuroscience she refers to is about explaining *success* (e.g., frog catching a fly), which is **different from "semantic evaluability" and the possibility of misrepresentation** that comes with the intentional notion of representation. |
| | John Krakauer | Disagrees with Eduard, stating that neuroscientists are **"infected by the everyday mental representation language"** and don't genuinely distinguish meanings. Argues that the "infestation" (using "representation" for informational content) is more likely in cognitive tasks (prefrontal cortex) than sensory-motor ones (spinal cord), even if the underlying neuroscience is the same. |
| | Francis Egan (Frankie) | Reaffirms that representational talk, by attributing content, **"highlights a particular aspect of that complex causal story"** that is salient given the explanatory target, serving a pragmatic purpose for simplification. |
| | Eduard Machery | Argues Frankie gives neuroscientists too much credit; he believes the notion of "representation" is **"much vagger and looser," used with "so little content"**. Agrees with John's empirical prediction that the frequency of the word "representation" might vary across brain regions or experimental traditions. |
| **"Shape" of Neural Activity & Generational Differences** | Paul Middlebrooks | Proposes that some neuroscientists, especially "younger generation," might use "representation" simply to describe the **"shape of whatever neural activity they're measuring"** (e.g., manifold as a representation of neural data), without linking it to mental representation. |
| | Eduard Machery | While the data is too small to definitively look at age/sub-discipline variation, he agrees it's an **interesting question**. |
| | Rosa Cao | Speculates that this usage is tied to the subfield (e.g., computer science tradition) rather than age. In computer science, "representation" can mean anything that carries information, like **any activation in a deep neural net**, motivated by talking about "information transformation" rather than explaining "representation hungry activity". |
| **What's At Stake: Misleading Language & Explanation** | Luis Favela | Expresses concern that using "mental kinds of words to describe non-mental phenomenon" (e.g., AI being "creative" or "hallucinating") is problematic because it **misleads the public and funding agencies**, importing mental talk to non-mental systems. |
| | John Krakauer | Agrees with Luis, citing examples where psychological concepts (like "decision making") are wrongly attributed to neural phenomena (like "manifolds"). Dislikes calling artificial units "neurons" for similar reasons. |
| | Eduard Machery | Points out that the panel is a **"biased sample"** as they largely agree on skepticism towards "neural representations" in a strong sense, unlike many other philosophers and scientists. States that **neuroscientists themselves "care extensively"** about the meaning of "representation," as evidenced by frequent social media discussions. |
| | Francis Egan (Frankie) | Agrees that the main stake is the **misleading conclusions philosophers (and others) draw about progress in understanding mental capacities** (consciousness, rational decision-making) when the same vocabulary is used for different things. Philosophers are "guilty" of appealing to neuroscientific work to support theories of content as if it solves intentionality. |
| **The Neural Story for Mental Representation** | John Krakauer | Believes neural evidence can **"update our theories algorithmic theories of what's going on in representation-rich behavior,"** but the ultimate explanation won't be articulated in neural language. Uses the computer/Word document analogy: information is *transformed* into a representation *at the moment of use*; neuroscience should explain *how* this transformation happens, not claim the information itself *is* the representation. Argues that **representation is a "process" that "happens at the moment of use,"** evident in actions like drawing a map. |
| | Rosa Cao | Counters John's computer analogy, arguing that if information is **"structured" and in a "standard format for retrieval,"** it can be considered a "non-current representation" even when not actively used. This is distinct from unstructured information. |
| | Eduard Machery | Disagrees with John's computer analogy, stating that computers *do* have representations at the hardware level through compiling high-level language into machine language. Argues that **brains differ from computers because they lack this "compiling"**. Finds John's definition of representation as only "at the moment of use" a "stipulation". |
| **Role of Neuroscientists & Levels of Explanation** | Luis Favela | Questions John's view of neuroscientists' role, suggesting he sees them as "useless" for explaining psychological states, acting like "methodological solipsists". |
| | John Krakauer | Counters that **psychology is an "effective theory" for mental phenomena**, similar to economics or sociology, which have their own primitives and ontology without needing lower-level explanations (e.g., particle physics for stretch reflex). Neural evidence can provide "confirmatory updating evidence" to break ties in psychological theories, but **explanations will remain in psychological primitives**. Believes there should be a "two-way street" where psychology also informs neuroscience. States that while lesions show "causal consequence," they don't "richly explain" language computation. |
| | Eduard Machery | Emphasizes the importance of **"levels of explanation"** and how notions like "representation" connect or disconnect them. Points to physics as an example where models at different scales are related, but **explanatory primitives remain at their own scales**. Suggests that "formal derivation on assumptions" is a way models at different scales can be understood. |
| **Unconscious Representations & Psychology's Scope** | Eduard Machery | Argues that much of psychology is concerned with **unconscious representations** (e.g., in reading research) that are not tied to conscious experience. Distinguishes this from John's focus on first-person conscious experience. |
| | John Krakauer | Clarifies he has **"no problem with there being unconscious representations"** (e.g., language of thought) but distinguishes them from "implicit policies". Argues that neuroscientists often use "representational language" across the brain (V1 to prefrontal cortex) without distinguishing between sensation (unconscious) and perception/representation (which can also be unconscious, like blindsight). |
| **Frankie's Pragmatic View of Mental Processes** | Francis Egan (Frankie) | Reaffirms that **"representational talk and content attribution is always pragmatically motivated,"** both in science and everyday life. We **model inaccessible mental states and processes using public/linguistic terms** (e.g., beliefs as inferential processes), allowing us to understand and evaluate them for accuracy. This serves simplification and communication of private experiences. |
| **Representational Behavior (Continued)** | Francis Egan (Frankie) | Confirms that drawing a picture is **"public representation"**. While the internal processes are characterized representationally, "representation has its home in kind of public representation in language" and is used to talk about non-public things. |
| | John Krakauer | Agrees that abilities like drawing are **unique "representational abilities"** that can be lost with brain injury. Reaffirms that the word "representation" should be for *this ability*, not for the neural explanation itself. Explains that **mental representations have properties of external ones**, allowing us to "operate on" them mentally like amending a drawing. Considers this **"our superpower"** (e.g., Gaudi imagining Sagrada Familia) and defends the concept of representation as being as powerful as language. |
| | Eduard Machery | Agrees that imagination is an "interesting capacity" with similarities to external representations but questions if *everything* John calls representational has these rich features. |
| **All Mental Life Representational?** | Luis Favela | Asks whether *all* mental life is expressible in representational terms, or if non-representational features exist. |
| | Francis Egan (Frankie) | Clarifies that mental capacities are *modeled* as linguistic processes, not that they *are* linguistic. |
| | John Krakauer | Believes that to the degree thoughts can be expressed in external representations (words, drawings, math), they have **internal analogues that are themselves representational**. He's not a "completist" but sees a strong connection. |
| **Closing Thoughts & Progress Made** | Rosa Cao | Found John's view clearer and "quite reasonable" after elaboration. Remains **resistant to "legislating how people use the term representation"** due to the diversity of enterprises. Emphasizes the importance of **getting clear on specific uses and avoiding "promissory note" explanations**. Mentions ongoing collaborative work creating a **"taxonomy of different uses"** where a commonality is that representations "have to be used and they have to be usable". |
| | Francis Egan (Frankie) | Agrees with Rosa's points and looks forward to her work; learned a lot from the discussion. |
| | John Krakauer | Believes his concerns about the term are "somewhat valid". Sees the value of mental representations as "effective theories" in cognitive science without neurons. Views neural evidence as **"confirmatory" for mental representation behavior**, but fears the use of "representation" for neural data leads to "deep conceptual confusions" about linking neuroscience and psychology. States he would "never police" the term. |
| | Eduard Machery | Highlights the **"great agreement" among the panelists on skepticism towards naive neural representation** and simplistic links between neuroscience and psychology, noting this consensus is not widespread. Expresses a differing view on the **"value of having imprecise and loose concepts,"** arguing they are "fundamental" and functional for science by allowing knowledge to circulate, even if it breeds confusion. Has changed his philosophical stance to support this view. |
| | Luis Favela | Jokes about being an "emotional vampire" but agrees the conversation highlighted how people "talk past each other". Argues against the "thousand flowers bloom" approach, preferring to **"weed out the weeds" and facilitate "good flowers"**. Reiterates his preference for a **"scorch earth" elimination of "representation"** or using more specific terms. |

## 2025-07-27 20:41:33

Two views on the cognitive brain
David L. Barack and John W. Krakauer
Abstract | Cognition can be defined as computation over meaningful
representations in the brain to produce adaptive behaviour. There are two views
on the relationship between cognition and the brain that are largely implicit in the
literature. The Sherringtonian view seeks to explain cognition as the result of
operations on signals performed at nodes in a network and passed between them
that are implemented by specific neurons and their connections in circuits in the
brain. The contrasting Hopfieldian view explains cognition as the result of
transformations between or movement within representational spaces that are
implemented by neural populations. Thus, the Hopfieldian view relegates details
regarding the identity of and connections between specific neurons to the status
of secondary explainers. Only the Hopfieldian approach has the representational
and computational resources needed to develop novel neurofunctional objects
that can serve as primary explainers of cognition

## 2025-07-28 19:47:59

Brain-inspired AI beats LLMs with lean logic
A Singapore startup just threw a sharp curve at the LLM status quo. Sapient Intelligence‚Äôs new Hierarchical Reasoning Model (HRM) ditches token-by-token ‚Äúthinking out loud‚Äù for something closer to how humans really solve problems: deep, silent, abstract reasoning. HRM splits the job between two brain-inspired modules: a slow planner that maps strategy and a fast solver that handles gritty details.

## 2025-07-28 19:48:10

Meta announces former GPT-4 co-creator as Superintelligence Labs Chief Scientist
Shengjia Zhao, one of the minds behind GPT‚Äë4 and OpenAI‚Äôs ‚Äúo‚Äëmodels,‚Äù has been appointed Chief Scientist at Meta‚Äôs new Superintelligence Labs. Zhao, who helped design ChatGPT‚Äôs first training pipeline and scaled large‚Äëlanguage model optimization, will work alongside Mark Zuckerberg and Alexandr Wang (ex‚ÄëScale AI). His hire hints Meta isn‚Äôt chasing just bigger models, but deeper reasoning and structured cognition, key to move from fluent text generators to systems that actually reason.

## 2025-07-28 22:17:54

LLM Embeddings Explained:
A Visual and Intuitive Guide
The embedding atlas of 50 random words and their closest tokens in the embedding space of `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B`.

Authors
Hesam Sheikh Hassani
Affiliation
University of Bologna
Published
March 28, 2025

## 2025-07-28 22:18:03

https://huggingface.co/spaces/hesamation/primer-llm-embedding?section=what_are_embeddings?

## 2025-07-31 23:06:12

Let‚Äôs begin with the sort of influential, Classical Autonomist Picture that
was defended by Jerry Fodor (e.g., 1997) and his collaborators, among
many others:
2 Neurocognitive Foundations of Mind
A Classical Autonomist Picture of the Mind
(1) To perceive, think, and act, minds represent the world (including the
organism) and compute over such representations.
(2) Mental representation and computation are theoretical posits within
psychological (not neuroscientific) models, which explain perception,
thought, and action autonomously from neuroscience.
(3) Mental representation and computation are modeled after representation and computation within (digital) computers.
(4) Representation and computation within (digital) computers are autonomous from physical implementation. Therefore, mental processes are
autonomous from neural implementation.
Note that there are at least two places where this Classical Picture alleges
autonomy from neuroscience: via supposedly autonomous psychological explanation and via supposedly autonomous representations and
computations.
The presumed nemesis of the Classical Picture is Connectionism (e.g.,
Rumelhart et al. 1986). Perhaps surprisingly, Connectionism can and often
has been pursued in a similarly autonomous way (e.g., Weiskopf 2011;
cf. several essays in Kaplan 2018). Here is a Connectionist Autonomist
Picture that begins with th

## 2025-08-01 18:52:24

On language and connectionism: Analysis of a parallel distributed processing model of language acquisition
Author links open overlay panel
Steven Pinker *
, 
Alan Prince *

Show more

Add to Mendeley

Share

Cite
https://doi.org/10.1016/0010-0277(88)90032-7
Get rights and content
Abstract
Does knowledge of language consist of mentally-represented rules? Rumelhart and McClelland have described a connectionist (parallel distributed processing) model of the acquisition of the past tense in English which successfully maps many stems onto their past tense forms, both regular (walk/walked) and irregular (go/went), and which mimics some of the errors and sequences of development of children. Yet the model contains no explicit rules, only a set of neuronstyle units which stand for trigrams of phonetic features of the stem, a set of units which stand for trigrams of phonetic features of the past form, and an array of connections between the two sets of units whose strengths are modified during learning. Rumelhart and McClelland conclude that linguistic rules may be merely convenient approximate fictions and that the real causal processes in language use and acquisition must be characterized as the transfer of activation levels among units and the modification of the weights of their connections. We analyze both the linguistic and the developmental assumptions of the model in detail and discover that (1) it cannot represent certain words, (2) it cannot learn many rules, (3) it can learn rules found in no human language, (4) it cannot explain morphological and phonological regularities, (5) it cannot explain the differences between irregular and regular forms, (6) it fails at its assigned task of mastering the past tense of English, (7) it gives an incorrect explanation for two developmental phenomena: stages of overregularization of irregular forms such as bringed, and the appearance of doubly-marked forms such as ated and (8) it gives accounts of two others (infrequent overregularization of verbs ending in t/d, and the order of acquisition of different irregular subclasses) that are indistinguishable from those of rule-based theories. In addition, we show how many failures of the model can be attributed to its connectionist architecture. We conclude that connectionists' claims about the dispensability of rules in explanations in the psychology of language must be rejected, and that, on the contrary, the linguistic and developmental facts provide good evidence for such rules.

## 2025-08-03 14:02:37

Algorithmic reasoning in
large language models and
neuro-symbolic architectures
by
Flavio Petruzzellis
Submitted to the Department of General Psycology
on September 30, 2024 in partial fulfillment of the requirements for the degree of
BRAIN, MIND & COMPUTER SCIENCE
ABSTRACT
Rule-based systems from the tradition of artificial intelligence possess reliable reasoning
capabilities, but struggle with flexibility and scalability. Conversely, modern deep learningbased systems have demonstrated impressive performance in a wide variety of tasks requiring
‚Äúfast thinking‚Äù, but lack robust reasoning capabilities. Motivated by the need to overcome
these limitations, this thesis investigates the design of AI systems that can learn to reason
systematically. Drawing from a rich tradition of AI and cognitive science, we leverage the
concept of compositionality as a common thread in our research. The contributions we offer
are twofold. First, we investigate the systematic reasoning capabilities of Large Language
Models probing them on the simplification of nested mathematical formulas, a problem that
reflects key properties of compositionality. Our experimental results on Llama 2 and GPT
models reveal that while scaling LLMs brings some improvements, their systematic reasoning
capabilities remain limited, even with specialized prompting methods like chain-of-thought
reasoning. Second, we propose a neuro-symbolic framework designed to learn and execute
convergent term rewriting systems, which can formally describe simple algorithms for the
iterative simplification of nested mathematical formulas. The framework is implemented in
two variants: NRS and FastNRS. We benchmark these implementations against the Neural
Data Router, OpenAI‚Äôs GPT-4 and o1-preview, demonstrating the robust systematic reasoning capability of our neuro-symbolic approach, while critically assessing the impact of
our design choices on efficiency and their limitations

## 2025-08-05 14:40:34

Cluster 0: Language, Meaning, and Thought
Tarski-based theory of meaning Dummett

The Language of Thought

Meaning, Language, and Natural Information

Meaning, Use, and Language-Games

Millikan: Language, Thought, and Biological Categories

Cluster 1: Representations and Cognition
Representations as Rate-Distortion Sweet Spots

Millikan Pushmi-Pullyu Representations

Mental Representations Smortchkova Papineau Bergman

Egan Deflating Mental Representation

Meaning from Matching: Correspondence and Representation

Concepts as the Interface

Mental Representation: Grounded in Rationality and Misrepresentation

Smortchkova Mental Representations: Theories, Deflationism, and Cognitive Science

Shea, Adams on representations

representations Ramsey etc

Cluster 2: Information Theory & Computation
COVER Information Theory and Machine Learning Algorithms

Basic processing unit: Shannon Channel

Semantic Variational Bayes: Solving Latent Variables with Information Theory

A Multifaceted Model of Information Theory

Reliable Circuits Using Less Reliable Relays

Evaluating Information in Cognitive Explanations

Evaluating Information in Explanations of Cognition

Mi≈Çkowski Correspondence Theory of Semantic Information

Natural Information and Representations

Cluster 3: Brain, Neuroscience, and Neural Networks
Churchland on Connectionism

hippocamp representations

NN Manifolds

Ned Block on Consciousness, Computation, and Meat

How Neurons Mean: Neurocomputation and Neurosemantics

Plate - Convolution, Superposition, and Distributed Representations

Predictive Coding vs Back Propagation

Evolution of Spiking Neural Networks

Information Capacity Shapes Human Intelligence

NMO2025 Rethinking Brain Plasticity and Functional Localization

Sparse Distributed Memory

Transformational Abstraction in Deep Convolutional Neural Networks

Cortical Neurons as Deep Artificial Neural Networks

Chirrimuta Minimal Models & Neural Computation: Distinct Explanations in Neuroscience

Cluster 4: Philosophy of Mind & Cognitive Science
Fodor's Asymmetric Dependency Theory

Explaining Inductive Concepts Through Causal Metasemantics

Teleosemantics and Indeterminacy: Martinez and Artiga

In Defense of Proper Functions: Millikan's Philosophy of Science

Functions: An Etiological Approach

Functions: Etiological Versus Propensity Theories

Normativity and Naturalist Theories of Judgement

Natural Information and Intentional Representation

Nizami - Critique of Information Theory Abuses in Neuroscience

Churchland

Rathkopf What Kind of Information is Brain Information?

Cerebr√≥polis: A Journey Through the Inner Workings of the Brain

Cluster 5: Complexity, Systems, Evolution
Glennan Rethinking Mechanistic Explanation: Complex Systems and Causation

Understanding Complexity: Systems, Chaos, and Evolution

Real Patterns: Complexity, Ontology, and Scientific Realism

From Bacteria to Bach and Back: Evolution of Minds

Sloman Evolution and Development of Biological Phenomena

Signals: Evolution, Learning, and Information

Biolinguistics as a Complex Biological System

on information functional, biological, Mann, Deacon, Gecow, MacKay, Cao etc

Cluster 6: Artificial Intelligence and Large Language Models
Almost every LLM is grounded (and always was)

Social Conventions and Bias in LLM Populations

LLMs and understanding

Fran√ßois Chollet on Symbolic Reasoning and the Future of AI

META: Large Concept Models: Training, Inference, and Applications

GPT-4 and the Turing Test

AI Revolution: Unstoppable Technological Advancement

Cluster 7: Semantics, Logic, and Formal Systems
Kripke's Completeness Theorem in Modal Logic

Model Theory and the Foundations of Logic

Logicality and Meaning: A Model-Theoretic Account

Gettier and impossibility of perfect knowledge

Penrose on G√∂del's Incompleteness Theorems

Cluster 8: Statistical and Learning Models
Statistics textbooks Tsilkiltis etc

Vector Space Models of Semantics

Piantadosi etc

Moderate Empiricism and Deep Learning: A Philosophical Reappraisal

Sequence to Sequence Learning with Neural Networks

IT textbooks

Cluster 9: Miscellaneous / Others
Futrell Efficiency in Human Language

Bickhard

Donald MacKay

Sayre

Summers_Stay

KEEP notes

Tatra National Park: Nature, Culture, and Legacy

REASONING

Chronicle of a Death Foretold

On formatlessness

Poland's EU Presidency: A Cultural Strategy

Tim Maudlin on QM interpretations

## 2025-08-05 16:55:48

Source: Excerpts from "Two views on the cognitive brain -- David L. Barack; John W. Krakauer -- Nature Reviews Neuroscience, 22, 2021 -- Springer Science and Business Media -- 10_1038_s41583-021-00448-6 -- 2224fda55e6e213baf7c54c7c9b52e3f -- Anna‚Äôs Archive.pdf"

I. Executive Summary

This paper by Barack and Krakauer delineates two contrasting perspectives on how the brain gives rise to cognition: the Sherringtonian view and the Hopfieldian view. The core disagreement lies in what constitutes the "first-level explainers" of cognitive phenomena. The Sherringtonian view, rooted in explanations of sensorimotor reflexes, emphasizes individual neurons and their specific, weighted connections within circuits. In contrast, the Hopfieldian view, drawing inspiration from distributed neural networks, posits that cognition emerges from transformations within or between high-dimensional "neural spaces" implemented by neural populations, relegating individual neuronal details to a secondary explanatory role. The authors argue that while the Sherringtonian view has dominated neuroscience, the Hopfieldian view offers superior representational and computational resources to explain complex cognitive phenomena, hinting at a potential "revolution" in cognitive neuroscience.

II. Introduction to Cognition and Representation

Definition of Thought/Cognition: The authors define cognition as "computation over representations to yield behaviour." This includes mental phenomena such as "Decision-making, planning, belief, recall, reasoning."
Fundamental Insight: A key insight is that these mental phenomena "are about something," implying a robust notion of representation is crucial.
Deficiency of Current Neuroscience: Contemporary cognitive neuroscience, particularly animal model studies, often models cognitive explanations on simpler "sensorimotor phenomena such as reflexes," which the authors argue "ignores the full implications of representational components to cognition."
Stronger Notion of Representation: For cognition, the authors advocate for a "more elaborate and restrictive notion of representation." Such representations possess specific properties:
Content: They "have content ‚Äî they are about something."
Evaluability: They are "evaluable, such as for truth, success, accuracy and the like."
Detachability: They are "capable of existing in the absence of their typical causes." For example, a true representation of an airplane can occur "when planning a trip, imagining the plane flight or telling a story about last summer‚Äôs vacation to Hawaii," not just when an actual airplane is present.
Combinability: They "can be combined and interact in various systematic ways."
System Production/Use: They are "produced and used by the system in order to generate behaviour."
Semantic Representations: Representations matching this understanding are termed "semantic representations" carrying "semantic contents."
Computation: Beyond representation, "Computation includes how representations are transformed, updated, created or deleted." These transformations are "essential for understanding cognition" and "must be identified and their neural realizations described."
III. The Sherringtonian View

Core Principle: This view explains cognition as the result of "operations on signals performed at nodes in a network and passed between them that are implemented by specific neurons and their connections in circuits in the brain." It maintains that "cognition will be explained just as Sherrington explained reflexes."
First-Level Explainers: For the Sherringtonian view, "neuron to neuron connections and the computations performed by these neurons or the traceable circuit in which they are embedded are the first-level explainers of cognition." (Box 1: Levels of explanation).
Implementational Level: Describes "in biophysical and physiological terms, the neurons and connections that realize a cognitive phenomenon," including "specific neural transfer functions" and details about "neurotransmitters" and "local circuit connections."
Algorithmic Level: Appeals to "computations performed by networks of nodes with weighted connections between them," where "nodes are neurons and these connections are synapses." It assumes "cognition is as amenable to a Sherringtonian form of algorithmic abstraction as are reflexes." A notable example is the "canonical cortical microcircuit."
Illustration (Motion Perception): Explained by neurons in area V5/MT responding to motion, then projecting to the lateral intraparietal area (LIP), where neurons integrate this evidence to make a decision. This is seen as a "series of processing steps each performed by individual neurons that are appropriately connected together."
Problems:XOR Problem/Combinations of Stimulus Properties: Cannot easily explain "exclusive or (XOR)" logical operations with simple linear feedforward networks, requiring dedicated circuits (a "giant look-up table") for every possible combination, which is "wildly impractical." While single neurons can conceptually perform XOR, the issue is scalability and the need for complex, non-linear functions at the single-neuron level.
Population-Level Phenomena: While the Sherringtonian approach can accommodate population codes if "each neuron is committed to a particular role in the circuit," it still insists "the key explanatory work is performed by single neurons and their interconnections."
Representational Constraints:Dedicated Circuits for Each Semantic Content: If each node has distinct semantic content, every variation in behavior caused by different semantic content requires a "distinct circuit," leading to an "explosion in circuit resources."
Complexity of Semantic Organization: Assigning multiple contents to a single node (e.g., firing rate range meaning "bear") increases semantic complexity, which "may not be easily usable by the brain."
Complex Contents (Grandmother Cell Hypothesis): Assigning complex contents (e.g., "snow is white") to a single node amounts to a "radical grandmother cell hypothesis" and "strains credulity." If combinations give rise to complex meanings, the "extended meanings are not represented in the brain," which is "wildly implausible" for phenomena like language.
Computational Constraints: Only "nodes and connections" are available at the algorithmic level, limiting computation to "transfer functions in the individual nodes." This restricts computation to local representations and prevents explanations based on "the action and interaction of multiple components."
IV. The Hopfieldian View

Core Principle: Emphasizes the "distributed nature of computation for cognition in neural systems," couching operations and representations "in terms of transformations between neural spaces."
First-Level Explainers: "Properties of neural spaces" are the primary explainers of cognition (Box 1).
Implementational Level: Describes "massed activity of neurons" by a "neural space that has a low-dimensional representational manifold embedded within it." Details of "biophysiological description of neurons or their detailed interconnections" are secondary or omitted. "Single neurons can play a role only as second-level explainers."
Algorithmic Level: Consists of "representational spaces as the basic entity and movement within these spaces or transformations from one space to another as the basic operations." Representations are "basins of attraction in a state space." Computations are described by "dynamical features of representational spaces such as attractors, bifurcations, limit cycles, trajectories and so on."
Illustration (Delayed Recall Task/Mixed Selectivity): Analysis of neural recordings from the lateral prefrontal cortex showed that "dimensionality of the population activity" (the number of independent ways the activity can vary) was higher on correct trials than error trials, especially when "non-linear mixed selectivity" was included. The "collapse of the dimensionality of the population on error trials reflects a reduced representational capacity," explaining the errors. This is a prime example of the Hopfieldian approach.
Benefits:Explanatory Resources: Provides explanatory resources unavailable to the Sherringtonian view, especially for "non-linear mixed-selectivity neurons" that give rise to "more complex representational spaces."
Accommodation of Single Neuron Data: Can accommodate "grid-cell responses" by arguing they "are the result of population-level processing that sculpts their activity by training and feedback." Single neuron responses are "explanatorily derivative" and have power "only because they are part of a larger population."
Invariance to Specific Neurons: "Trajectories through state spaces are invariant to the specific neurons sampled from the population to generate them." Cognition is "a level above a one-to-one correspondence between the neural space and a particular pattern of connections between a specific set of identified neurons."
Flexibility: "More flexible both representationally and computationally" and its computational descriptions "subsume those of the Sherringtonian." Covers more systems because some cognitive systems may not satisfy point-to-point descriptions.
Neurofunctional Spaces: Introduces "new first-level explanatory objects" that possess "both functional and neural properties (neurofunctional properties)." Examples include movement planning explained as a "trajectory through a neural state space." These trajectories are "a form of behaviour" that are "representational in our more full-blooded sense."
Provides Understanding: Offers "intuitive visualization of the interactions" of neural dynamics, analogous to Feynman diagrams, which is "essential to scientific explanation."
V. Comparison and Potential Reconciliation

The "Neuron vs. Population Doctrine" Debate: The authors acknowledge that the debate can be framed as a clash between the neuron doctrine (single neurons as basic explanatory units) and the population doctrine (neural populations as central explanatory units). However, they "reject this rather pedestrian version of the debate."
Updated Views (Neuron+ and Population+ Doctrines):Sherringtonian (Neuron+): Acknowledges importance of populations but stresses that "the specific connections underlying those assemblies will inform their function." Population roles derive from "connections and transformations performed by their single-neuron constituents."
Hopfieldian (Population+): Acknowledges single neurons contribute to activity, but "specific connections between cells are irrelevant as first-level explainers of cognition." Single neuron activity is "assimilated" as "predominant drivers of population activity in particular contexts."
Are the Views Closer? The authors question if the views can be reconciled, leading to three possibilities:
Replacement: The Hopfieldian approach, facing intractable problems for Sherringtonianism (e.g., XOR, mixed selectivity), presents a "new population+ doctrine that will replace the older Sherringtonian approach."
Reduction: The Sherringtonian view attempts to "reduce the phenomena underlying the Hopfieldian approach to cell to cell connections," hoping that "for each way of performing a cognitive operation there will be a description in terms of neurons and local circuit connections." The authors express skepticism about this strategy due to the need to account for semantic components with "second-level explainers."
Reconciliation:Brain Area Differences: Different brain areas or evolutionary history might favor one view over the other. Older, modular structures (brainstem, spinal cord) might be Sherringtonian (computationally dedicated modules), while newer, flexible structures (cortex) might be Hopfieldian (flexible modules for a range of computations).
Common Ground/Functional Types: Seeking a "dictionary of functional types that might underlie neurocognitive computation" (e.g., Fuster's cognits, Wang's reverberatory dynamics) that are applicable to both individual neurons and populations. However, these would likely be "second-level explainers."
VI. The Case for a Hopfieldian-Led Revolution

Underlying Computational Entities: The authors argue for understanding that "there are underlying computational entities implemented by the brain." These implementations can take the form of either single neurons/specific connectivity (Sherringtonian) or mass activity in neuronal populations (Hopfieldian). Both are "instances of the implementation of a computation that is used to transform representations for behaviour."
Hopfieldianism as Springboard: The "characterization of this novel cognitive neuroscience has its best springboard in Hopfieldianism" because it "adopts fewer such constraints" than the Sherringtonian view.
Revolutionary Cognitive Neuroscience: This would involve:
Neural Data as Information Source: Neural activity becomes a "rich source of information along with behaviour to construct theories of cognition," emphasizing discovery of "cognitive computation directly from neural activity embedded in low-dimensional manifolds."
Quantitative/Conceptual Advances: Need for "a new theory of computation via these neural objects" (trajectories in state spaces), mathematically characterizing operations and relating them to ecological function.
Interaction between Views: Developing a "theory of the interface between the two types of circuit" as Sherringtonian circuits are relevant for inputs (sense organs) and outputs (spinal cord).
Deeper Truths and AI Implications: The Hopfieldian view may hint at "a deeper truth about cognitive function," where explanations require "state spaces across many spatio-temporal scales from the single molecule to the whole brain." These "dynamical objects" would be first-level explainers. This has implications for AI, suggesting that "other tissue properties may also be important in the construction of these novel explanatory objects," and theorists should "cast a wide representational and computational net, utilizing evidence at all levels from both brain and behaviour to draw conclusions about the nature of the mind."
NotebookLM can be inaccurate; please double check its responses.

## 2025-08-08 21:35:14

Thinking as Navigation in Embedding Space

Thinking is navigating in a high-dimensional space.

Meanings are embeddings in this space.

Navigation in ants (or simple agents) is not fundamentally different from this.

Proper semantics for modal logic can be described in terms of closeness between multimodal embeddings.

Shannon theory, the Asymptotic Equipartition Property (AEP), and typicality play a crucial role here.

Inference and reasoning reduce to simple linear algebra (the Mikolov effect).

Creating a concept corresponds to clusterization.

Meaning is objective‚Äîderived from observations of utterances (Frege was right).

Other inferences, such as abduction, require concept creation and thus go beyond pure navigation.

The generator of information in theories consists of axiom-clusters.

## 2025-08-13 00:34:18

argue that analyses of explanation must include refer-
ence to causal relationships if they are to distinguish good explanations

from bad. The difficulties that noncausal models of explanation (such as
the CL model (Hempel 1965), Philip Kitcher‚Äôs (1989) unification model,
and Paul Churchland‚Äôs (1989) representational model) have in delivering
the right verdicts on standard test cases argue collectively for a causal

approach to explanation in neuroscience.

## 2025-08-13 00:34:28

argue that analyses of explanation must include refer-
ence to causal relationships if they are to distinguish good explanations

from bad. The difficulties that noncausal models of explanation (such as
the CL model (Hempel 1965), Philip Kitcher‚Äôs (1989) unification model,
and Paul Churchland‚Äôs (1989) representational model) have in delivering
the right verdicts on standard test cases argue collectively for a causal

approach to explanation in neuroscience.

## 2025-08-13 00:42:59

https://doi.org/10.1007/s10670-021-00392-y
ORIGINAL RESEARCH
The Ups and Downs of Mechanism Realism: Functions,
Levels, and Crosscutting Hierarchies
Joe Dewhurst1 ¬∑ Alistair. M. C. Isaac2
Received: 12 December 2019 / Accepted: 1 March 2021 / Published online: 3 May 2021
¬© The Author(s) 2021
Abstract
Mechanism realists assert the existence of mechanisms as objective structures in the
world, but their exact metaphysical commitments are unclear. We introduce Local
Hierarchy Realism (LHR) as a substantive and plausible form of mechanism real-
ism. The limits of LHR reveal a deep tension between two aspects of mechanists‚Äô
explanatory strategy. Functional decomposition identifies locally relevant entities
and activities, while these same entities and activities are also embedded in a nested
hierarchy of levels. In principle, a functional decomposition may identify entities
engaging in causal interactions that crosscut the hierarchical structure of composi-
tion relations, violating the mechanist‚Äôs injunction against interlevel causation. We
argue that this possibility is realized in the example of ephaptic coupling, a subsidi-
ary process of neural computation that crosscuts the hierarchy derived from synaptic
transmission. These considerations undermine the plausibility of LHR as a general
view, yet LHR has the advantages that (i) its metaphysical implications are precisely
stateable; (ii) the structure it identifies is not reducible to mere aggregate causation;
and (iii) it clearly satisfies intuitive and informal definitions of mechanism. We con-
clude by assessing the prospects for a form of mechanism realism weaker than LHR
that nevertheless satisfies all three of these requirements.

## 2025-08-13 00:45:53

An option space for early neural evolution
Ga ÃÅspa ÃÅr Je ÃÅkely1

, Fred Keijzer2 and Peter Godfrey-Smith3,4

1
Max Planck Institute for Developmental Biology, Spemannstrasse 35, Tu Ãàbingen 72076, Germany
2
Department of Theoretical Philosophy, University of Groningen, Oude Boteringestraat 52, Groningen 9712 GL,
The Netherlands
3
Philosophy Program, The Graduate Center, City University of New York, New York, NY 10016, USA
4
History and Philosophy of Science Unit, University of Sydney, Sydney, New South Wales 2006, Australia
GJ, 0000-0001-8496-9836
The origin of nervous systems has traditionally been discussed within two
conceptual frameworks. Input ‚Äìoutput models stress the sensory-motor
aspects of nervous systems, while internal coordination models emphasize
the role of nervous systems in coordinating multicellular activity, especially
muscle-based motility. Here we consider both frameworks and apply them
to describe aspects of each of three main groups of phenomena that nervous
systems control: behaviour, physiology and development. We argue that
both frameworks and all three aspects of nervous system function need to
be considered for a comprehensive discussion of nervous system origins.
This broad mapping of the option space enables an overview of the many
influences and constraints that may have played a role in the evolution of
the first nervous systems.

## 2025-08-14 17:33:53

Mares Logic and Information
1 source
This Cambridge Elements publication, authored by Edwin Mares and part of the Philosophy and Logic series, explores the intricate relationship between logic and information. The Element focuses on two primary areas: utilizing logic to manage and interpret information, and employing the concept of information to interpret logical systems. It uniquely defines information to encompass misinformation and disinformation, using this broad definition to analyze paraconsistent and relevant logics and their integration with informational updating and probability theory. The book also considers how information can be extracted from logical proofs and how logical truths themselves can convey information, even when their probability is zero in traditional models.

look for kind of information discussed along the text

The text discusses several kinds of information, often in contrast to one another or as different aspects of a unified concept:
‚Ä¢ General Concept of Information: The Element defines 'information' broadly to include both misinformation and disinformation. This is a key departure from some other theories, like Luciano Floridi's, which stipulate that information must always be true. The author argues that including false forms of information is essential for the usefulness of the logic of information, as logic can help eliminate them from one's information state. This broad concept also covers information gathered non-representationally, such as directly perceiving a fact. A piece of information is characterized by its appearance of grounding in facts, even if it is not actually grounded.
‚Ä¢ Conflicting Information: This is a significant phenomenon that presents difficulties for theoretical logicians. It arises when contradictory data is presented, such as two incompatible speed limit signs. The Element integrates paraconsistent and relevant logics to treat such conflicts more straightforwardly, as classical models struggle with consistency.
‚Ä¢ Partial Information: Our information about the world is almost always partial. The Element addresses this by using models that can represent partiality, such as supervaluational approaches or situation semantics, which focuses on "parts of worlds" (situations) rather than complete possible worlds. Kleene's three-valued semantics allows formulas to be true, false, or neither, effectively managing partial information.
‚Ä¢ Implicit and Explicit Information:
    ‚ó¶ Implicit information is understood in terms of unstructured propositions, often represented as sets of situations or possible worlds. An agent might possess information implicitly without being consciously aware of it or having extracted it.
    ‚ó¶ Explicit information is understood in terms of structured propositions, which contain individuals and properties as constituents and represent the manner in which information is naturally given. Logical proofs, for example, are seen as the extraction of explicit information from premises. The distinction is crucial for addressing the "scandal of deduction," where logical truths, though implicitly contained, provide new explicit information upon derivation.
‚Ä¢ Informational Content (Propositional Information): Information is generally considered propositional, meaning it conveys something about the world and can be expressed by language. This contrasts with the idea of "data" alone, which might be well-formed and true but not yet informative without a meaningful context. Propositions are language-neutral contents of sentences. The Element discusses information conditions for statements as being closely related to, but distinct from, truth conditions.
‚Ä¢ Information States: A key concept, representing the information an agent "has in their possession at a given time". These states can be internal (in one's mind) or external (in a database, notebook, or environment), aligning with the extended mind hypothesis. Information states can change over time and may contain conflicting information. The Element explores both the static aspect (logical principles governing closure) and the dynamic aspect (how states are updated with new information) of information states.
‚Ä¢ Upstream and Downstream Measures of Information (Quantity of Information):
    ‚ó¶ Upstream measures quantify the "difficulty of extraction" or making implicit information explicit. These are often based on complexity theory, such as the length of the shortest proof of a formula, measuring the information contained in logical theorems.
    ‚ó¶ Downstream measures quantify the difficulty of integrating a proposition into one's beliefs, preferences, and emotional attitudes. The Carnap-Bar Hillel (CBH) theory of information, based on probability, is a downstream measure, where surprising propositions (those with low prior probability) are considered to contain more information because they require more belief revision. Sperber and Wilson's relevance theory also considers downstream effects, where information is relevant if it causes positive cognitive changes.
‚Ä¢ Hard and Soft Information: This distinction relates to the reliability or "entrenchment" of information. "Harder evidence" is more resistant to undermining by new information, similar to how researchers in belief revision distinguish more strongly held beliefs through epistemic entrenchment.
‚Ä¢ Negative Information: On the incompatibility semantics, negative information (¬¨A) is treated as a form of positive information, meaning it holds when something incompatible with A obtains. It is distinguished from merely denying that information is held.
‚Ä¢ Common Information: This refers to information that is shared among agents, where everyone knows the information, and everyone knows that everyone knows it, and so on. This concept is important in multi-agent models and game theory

## 2025-08-14 20:22:16

A central problem highlighted is what Jaakko Hintikka called the "scandal of deduction". According to traditional semantic theories like the Carnap-Bar Hillel (CBH) theory and possible world semantics, logical truths (statements true in every possible world) carry no semantic information at all, assigning them a quantity of zero. This is counter-intuitive because people can learn logical truths and seemingly gain information from logical deductions. Furthermore, deductions from premises are not supposed to yield any extra information in these traditional views



But this is misunderstanding in few ways:
- learning is gaining information; 
- axioms GENERATE information - they emit inforamtion in colloraries 
- metarules emit information about ordering and set inference realtions between non-increasing entopry clusters

## 2025-08-16 12:52:24

Information in Explaining Cognition: How to Evaluate It?
Nir Fresco 1,2
1
2
Department of Cognitive and Brain Sciences, Ben-Gurion University of the Negev, Be‚Äôer Sheva 841050, Israel;
nfresco@bgu.ac.il
Department of Philosophy, Ben-Gurion University of the Negev, Be‚Äôer Sheva 841050, Israel
Abstract: The claims that ‚ÄúThe brain processes information‚Äù or ‚ÄúCognition is information processing‚Äù
are accepted as truisms in cognitive science. However, it is unclear how to evaluate such claims
absent a specification of ‚Äúinformation‚Äù as it is used by neurocognitive theories. The aim of this article
is, thus, to identify the key features of information that information-based neurocognitive theories
posit. A systematic identification of these features can reveal the explanatory role that information
plays in specific neurocognitive theories, and can, therefore, be both theoretically and practically
important. These features can be used, in turn, as desiderata against which candidate theories of
information may be evaluated. After discussing some characteristics of explanation in cognitive
science and their implications for ‚Äúinformation‚Äù, three notions are briefly introduced: natural, sensory,
and endogenous information. Subsequently, six desiderata are identified and defended based on
cognitive scientific practices. The global workspace theory of consciousness is then used as a specific
case study that arguably posits either five or six corresponding features of information.

## 2025-08-16 12:52:42

Information in Explaining Cognition: How to Evaluate It?
Nir Fresco 1,2
1
2
Department of Cognitive and Brain Sciences, Ben-Gurion University of the Negev, Be‚Äôer Sheva 841050, Israel;
nfresco@bgu.ac.il
Department of Philosophy, Ben-Gurion University of the Negev, Be‚Äôer Sheva 841050, Israel
Abstract: The claims that ‚ÄúThe brain processes information‚Äù or ‚ÄúCognition is information processing‚Äù
are accepted as truisms in cognitive science. However, it is unclear how to evaluate such claims
absent a specification of ‚Äúinformation‚Äù as it is used by neurocognitive theories. The aim of this article
is, thus, to identify the key features of information that information-based neurocognitive theories
posit. A systematic identification of these features can reveal the explanatory role that information
plays in specific neurocognitive theories, and can, therefore, be both theoretically and practically
important. These features can be used, in turn, as desiderata against which candidate theories of
information may be evaluated. After discussing some characteristics of explanation in cognitive
science and their implications for ‚Äúinformation‚Äù, three notions are briefly introduced: natural, sensory,
and endogenous information. Subsequently, six desiderata are identified and defended based on
cognitive scientific practices. The global workspace theory of consciousness is then used as a specific
case study that arguably posits either five or six corresponding features of information.

## 2025-08-16 12:52:46

6. Conclusions
Information is sometimes only an explanatory gloss rather than a key explanatory
construct in understanding cognition; but when information does play an important ex-
planatory role, it is methodologically useful to explicate that role. To that aim, having
distinguished amongst natural, sensory, and endogenous information, we have identi-
fied and defended six desiderata that information-based neurocognitive theories often
posit: (a) quantifiability, (b) substrate neutrality, (c) sender neutrality, (d) receiver dependence,
(e) symbolic/non-symbolic, and (f) mistaken tokening.

## 2025-08-16 12:58:27

Information as a Probabilistic Difference Maker
Andrea Scarantino
Pages 419-443 | Published online: 02 Feb 2015
Cite this article https://doi.org/10.1080/00048402.2014.993665 CrossMark LogoCrossMark
 
Sample our Humanities journals, sign in here to start your FREE access for 14 days
Taylor & Francis Online
Top
 Full Article
 Figures & data
 References
 Citations
 Metrics
 Reprints & Permissions
Read this article
Share
Abstract
By virtue of what do alarm calls and facial expressions carry natural information? The answer I defend in this paper is that they carry natural information by virtue of changing the probabilities of various states of affairs, relative to background data. The Probabilistic Difference Maker Theory (PDMT) of natural information that I introduce here is inspired by Dretske's [1981] seminal analysis of natural information, but parts ways with it by eschewing the requirements that information transmission must be nomically underwritten, mind-independent, and knowledge-yielding. PDMT includes both a qualitative account of information transmission and a measure of natural information in keeping with the basic principles of Shannon's communication theory and Bayesian confirmation theory. It also includes a new account of the informational content of a signal, understood as the combination of the incremental and overall support that the signal provides for all states of affairs at the source. Finally, I compare and contrast PDMT with other probabilistic and non-probabilistic theories of natural information, most notably Millikan's [2013] recent theory of natural information as non-accidental pattern repetition.

## 2025-08-16 21:04:45

INFORMATION AS A PROBABILISTIC
DIFFERENCE MAKER
Downloaded by [Selcuk Universitesi] at 00:58 07 February 2015
Andrea Scarantino
By virtue of what do alarm calls and facial expressions carry natural
information? The answer I defend in this paper is that they carry natural
information by virtue of changing the probabilities of various states of affairs,
relative to background data. The Probabilistic Difference Maker Theory
(PDMT) of natural information that I introduce here is inspired by Dretske‚Äôs
[1981] seminal analysis of natural information, but parts ways with it by
eschewing the requirements that information transmission must be nomically
underwritten, mind-independent, and knowledge-yielding. PDMT includes
both a qualitative account of information transmission and a measure of
natural information in keeping with the basic principles of Shannon‚Äôs
communication theory and Bayesian confirmation theory. It also includes a
new account of the informational content of a signal, understood as the
combination of the incremental and overall support that the signal provides
for all states of affairs at the source. Finally, I compare and contrast PDMT
with other probabilistic and non-probabilistic theories of natural information,
most notably Millikan‚Äôs [2013] recent theory of natural information as non-
accidental pattern repetition.

## 2025-08-17 18:54:58

‚Ä¢ Phototaxis in Sponge Larvae Sponge larvae are known to exhibit phototaxis, which is the movement towards or away from light.
‚Ä¢ Photosensitive Ciliated Cells This phototactic steering is achieved by individual photosensitive ciliated cells.
‚Ä¢ Direct Control Each of these cells uses light-controlled rudder-like cilia to enable steering. This means the sensory mechanism (light detection) directly influences the motor activity (ciliary movement) on the same cell.
‚Ä¢ Non-Neural Coordination This is a significant example of non-neural control of locomotion.
‚Ä¢ Efficiency and Neural Evolution This direct, cell-autonomous connection between sensory and motor capacities in sponge larvae is considered inefficient in terms of evolutionary development, as every motor component needs its own sensor. The proposed advantage of neurons, in this context, is to enable a small number of sensory cells to control a large bank of motor devices, thereby increasing efficiency.
‚Ä¢ Input-Output (IO) Behavior Hypothesis The sponge larva case serves as a prime example supporting an Input-Output (IO) behavior hypothesis for the early evolution of nervous systems, emphasizing how a system can process sensory information (input) to produce motor output, even without complex neural structures.

## 2025-08-18 10:48:47

Dendrophilia versus continuity in hierarchical reasoning
Abhishek Dedhe
Carnegie Mellon University, Pittsburgh, Pennsylvania, United States
Karishma Kulshrestha
Carnegie Mellon University, Pittsburgh, Pennsylvania, United States
Soham Kulkarni
Troy High School, Fullerton, California, United States
Steven Piantadosi
UC Berkeley, Berkeley, California, United States
Jessica Cantlon
Carnegie Mellon, Pittsburgh, Pennsylvania, United States
Abstract
Hierarchical reasoning might be qualitatively unique to humans or alternatively may arise from quantitative differences
in cognitive resources. We tested hierarchical reasoning abilities across adults, children, crows, and monkeys, evaluating
two hypotheses: the Strong Dendrophilia Hypothesis, which posits human uniqueness, and the Continuity Hypothesis,
which attributes differences to variations in information-processing capacity. Using Bayesian modeling, we found that
hierarchical reasoning (dendrocapacity) is not exclusive to humans, though the tendency to engage in it (dendropro-
clivity) varies across age and species. Adults exhibited the strongest dendroproclivity, while children and non-humans
showed graded performance influenced by cognitive resource demands and task complexity. Hierarchical mechanisms
such as Last-In-First-Out (LIFO) stacks were prevalent across groups. These findings challenge human uniqueness in
hierarchical reasoning and suggest its emergence through incremental increases in cognitive capacities across develop-
ment and evolution.6167
In D. Barner, N.R. Bramley, A. Ruggeri and C.M. Walker (Eds.), Proceedings of the 47th Annual Conference of the Cognitive Science
Society ¬©2025 the author(s). This work is licensed under a Creative Commons Attribution 4.0 International License (CC BY).

## 2025-08-18 13:28:27

We introduce FuzzyVis, a proof-of-concept system that enables intuitive and expressive exploration of complex ontologies. FuzzyVis
integrates two key components: a fuzzy logic-based querying model
built on fuzzy ontology embeddings, and an interactive visual interface
for building and interpreting queries. Users can construct new composite concepts by selecting and combining existing ontology concepts
using logical operators such as conjunction, disjunction, and negation.
These composite concepts are matched against the ontology using fuzzy
membership-based embeddings, which capture degrees of membership
and support approximate, concept-level similarity search.
‚àó
vladimir.zhurov@uwo.ca
‚Ä†
jkausch@uwo.ca
‚Ä°
kamrans@uwo.ca
¬ß
mostafa.milani@uwo.ca

## 2025-08-19 21:01:34

information is essential for the organization of life itself and for the propagation of complex organisms

## 2025-08-19 21:04:03

https://plato.stanford.edu/entries/information/

## 2025-08-20 12:46:39

biology and neuroscience, explain phenomena by uncovering mechanisms. What they meant by ‚Äúmechanism‚Äù is what Copeland meant: an organized system of components and activities such that the components and activi-ties, organized the way they are, produce the phenomenon (Bechtel and Richardson 1993; Glennan 1996; Machamer, Darden, and Craver 2000).

## 2025-08-20 12:48:42

philosophers have used an account of mechanisms associated with New Mechanism to identify the particular features of mechanisms that can perform physical computations. As Piccinini puts it, it has been a shift from computation explicating mechanism to mechanism explicating computation.

## 2025-08-20 12:51:17

To avoid unlimited pancomputationalism, several theorists introduced constraints on which mappings are acceptable. Perhaps the most popular constraint is a causal one. According to the causal account of physical computation, only mappings that respect the causal relations between the computational state transitions‚Äîincluding those that are not instantiated in a given computation‚Äîare acceptable (Chalmers 2011; Chrisley 1994; Scheutz 1999).

## 2025-08-20 13:08:49

. The Mechanistic Account.
3.1. Mechanistic Explanation. The central idea is to explicate computing mechanisms as systems subject to mechanistic explanation. By
mechanistic explanation of a system X, I mean a description of X in terms
of spatiotemporal components of X, their functions, and their organization, to the effect that X possesses its capacities because of how X‚Äôs
components and their functions are organized.4

## 2025-08-20 13:55:59

Lord Kelvin echoed this quantification credo: ‚ÄúWhen you can measure what you are speaking about, and express it in numbers, you know something about it‚Äù
oxfordreference.com
. Such views capture a widely held intuition: theories that yield numerical predictions and measurable parameters are often seen as superior to those that remain purely qualitative.

## 2025-08-20 13:56:39

Quantification enhances explanatory power by allowing exact comparison between theory and observation, and it boosts predictive accuracy by specifying how much of an effect to expect, not just whether it should occur.

## 2025-08-22 10:09:01

d something that does that‚Äô (Lewis 1970:22). Lewis and
his contemporaries proposed that intensional functions do what meanings do, and
higher-order logic in turn became the field‚Äôs most important toolkit.

## 2025-08-22 10:11:35

A case for deep learning in semantics: Response to Pater
CHRISTOPHER POTTS

## 2025-08-23 13:43:01

https://doi.org/10.1093/acprof:oso/9780199658855.001.0001
