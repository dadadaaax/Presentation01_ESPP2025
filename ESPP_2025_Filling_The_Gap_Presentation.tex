\documentclass[10pt, aspectratio=169, handout]{beamer}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{setspace}
\linespread{0.93} 
\usetikzlibrary{shapes.geometric, arrows, positioning, arrows.meta}
% \usepackage{neuralnetwork}
\usepackage[table]{xcolor}

\usepackage[absolute,overlay]{textpos}
\usetikzlibrary{decorations.pathreplacing, positioning}



\usepackage{caption} % Required for \captionof
\usepackage[T1]{fontenc} % Support for \k command
\usepackage[
    backend=biber, % Ensure biber is installed and used as the backend
    style=apa,     % APA citation style
    sorting=ynt    % Sort bibliography by year, name, title
]{biblatex}

\addbibresource{biblioteka.bib} % Ensure the file 'biblioteka.bib' exists in the same directory and contains valid entries
%Define theme and color settings
\usetheme{boxes}
\usecolortheme{dove} % Grayscale color theme
\usebeamerfont{serif}
% Remove navigation symbols
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{frametitle}{
    \vspace{0.2cm}
    \begin{centering}
        {\textbf{\Huge}\insertframetitle\par} % Increased font size
    \end{centering}
}
\setbeamercolor{frametitle}{fg=blue!80!black}
\setlength{\parskip}{0.3em}  % Adjust the value as needed
%\setbeamerfont{frametitle}{size=\normalsize, series=\bfseries}


\usepackage{calc} % for \widthof (if not already)
% Bottom-right note macro (adjust position/size as needed)
\newcommand{\brnote}[1]{%
  \begin{textblock*}{\widthof{#1}-1.6em}(0.85\textwidth,0.935\textheight)%
    \raggedleft\scriptsize #1%
  \end{textblock*}%
}






\title{Filling the Gap between Shannon Information \\ and Semantic Information}
\author{Daniel Piecka}
\institute{Institute of Philosophy and Sociology \\ of the Polish Academy of Sciences, Warsaw \\ \texttt{pieckadaniel@gmail.com}}
\date{\today} 
 



%*************************EXCLUDING FRAMES ***********************************
%\includeonlyframes{abstract_setup,first_causal_setup,shannon_pipeline_source_channel,fly_frog_pipeline, references}

%\includeonlyframes{carcassi_entropy,references}
%\includeonlyframes{fly_frog_pipeline,basic_topologies, references}


\begin{document}


\begin{frame}[label=title]

  \maketitle
\end{frame}


\begin{frame}[label=overview]{My talk today}
    \begin{enumerate}
        \item \textbf{Kinds of information} Why suspect there are multiple kinds of information?
        \item \textbf{The pipeline: Shannon  model in the eyes of New Mechanist}
        \item \textbf{Other examples} Kinds of (cognitive) pipelines
        \item \textbf{Structural representations:} A candidate for bridging compositional
        \item \textbf{Compositionality:} The missing piece in structural reps
        \item \textbf{Proposal:} Noisy addition as a pathway to linearity (multidimentional spaces)
        \item \textbf{Conclusions:} One kind of information, many kinds of pipelines 
    \end{enumerate}
\end{frame}



\begin{frame}[label=gap] {(Some)kinds of information (after 1948)}
    
    
   
        \begin{tikzpicture}
            \node[fill=blue!40, ellipse, inner sep=3pt] at (-3, 0) {Shannon information};
             \node[fill=blue!40, ellipse, inner sep=3pt] at (3, 0) {Fisher information};

            \node[fill=green!20, ellipse, inner sep=3pt] at (2, -2) {Semantic information};  
            \node[fill=gray!20, ellipse, inner sep=3pt] at (2, +2) {Strongly semantic information};

            \node[fill=yellow!20, ellipse, inner sep=3pt] at (-2, -1) {Natural information};
            \node[fill=red!20, ellipse, inner sep=3pt] at (1, -2) {Non-natural information};

            \node[fill=purple!20, ellipse, inner sep=3pt] at (-3, 2) {Biological information};
            \node[fill=orange!20, ellipse, inner sep=3pt] at (3, -1) {Mental representation};

            \node[fill=cyan!20, ellipse, inner sep=3pt] at (-1, 3) {Neural information};
            \node[fill=lime!20, ellipse, inner sep=3pt] at (0, -3) {Functional information};
            \node[fill=pink!20, ellipse, inner sep=3pt] at (4, -3) {Non-semantic information};
        \end{tikzpicture}
 
        ...and they can intersect. 

\end{frame}

\begin{frame} {A Graded Taxonomy proposal }
    
In the result we end with potencially rich and open set of the kinds of information. 
\cite{fresco_functional_2018}

\end{frame}


\begin{frame} {First claim}
    \begin{center}
        \Large This, I think, makes a place for a blow up of the ontology of cognitive science, especially when it is trying hard to use Shannon theory of information for its explanatory goals.

    \end{center}
\end{frame}


\begin{frame}[label=abstract_setup] {First abstract setup: two random variables: \(X\) and \(Y\)}


Let \(X\) - (Source) and \(Y\) (Destination) be two random variables ranging over collections.
Both can be interpreted in many ways, but for cognitive science applications Source will be an external (internal) source having some variability $H$ (entropy) and Destination a collection of possible objects, such as types, actions, etc.

In any abstract setup there always is symetrical measure of information relation between the two.
% --- Form 2 ---
Mutual information $I(X;Y)$ as reduction in uncertainty
\[
I(X;Y) \;=\; H(X) - H(X|Y) \;=\; H(Y) - H(Y|X)
\]
Mutual information is the drop in entropy of one variable once the other is known.

\emph{\textbf{This way of speaking about introduces the psychological aspect of theory of information (uncertainty , surprise, knowledge)
}}
% --- Form 4 ---
Mutual information as expectation over the joint distribution
\[
I(X;Y) \;=\; \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} 
p(x,y)\, \log \frac{p(x,y)}{p(x)\,p(y)}
\]
Computed directly from the joint and marginal probabilities of the variables.


\end{frame}


\begin{frame}[label=carcassi_entropy] { Start from the measure: Carcassi interpretation of entropy}

   Entropy is well established measure of information defined by Claude Shannon 
    \cite{shannon1949mathematical} as the average amount of information  produced by / stored by a stochastic source. 

    Variability interpretation of entropy \cite{carcassi_variability_2021}  is better starter for cognitive science interpretations. 


"Shannon entropy can be fully understood as measuring the variability of the elements within a given distribution", what gives it "a crisp intuitive meaning that is general and applicable to all branches of science" (\cite{carcassi_variability_2021}).

\textbf{This view leads to ontologically and epistemologically neutral notion of information.
}

\end{frame}

\begin{frame}[label=carcassi_entropy2] { Start from the measure: Carcassi interpretation of entropy (cont.)} 

\textbf{In this view elements of the collections are not necessarily "messages" bearing any meanings.} This we get rid of  the whole branch of problems summoned by seeing Shannon theory as nonstarter for the foundation of explanantions in cognitive science  (antiencodingism arguments c.f. \cite{bickhard_interactivist_2009} ) We also dismiss circularities in psychological notion of "uncertainty", pervasive in literature (epistemological neutrality)

This view does not imply pancomputationalism but rather provides a non-psychological and non-semantic interpretation of information theory, as the elements of the collections are not necessarily "messages."

E.g. animal signalling can be described this way as as reacting on the variablilityby the pipeline resuting in behavior (vervet but also cnidarians )


Non-psychological interpretation: we do not say anymore in any substantial way about (average) surprisal / uncertainty of event.
 

\end{frame}




\begin{frame} [label=first_causal_setup] {First causal  setup of pipeline  }
To model some aspects of cognition (e.g perception) the abstract pair of random variables of \(X\) and \(Y\) is interpreted causally:

\textbf{There is a causal relationship between the source and the destination, i.e. the is a directed  physical (e.g. biophysical) relationship between the two.} 

This causal, mechanists interpretation immediately solves the symmetry problem appearing: the destination information is such and such because the source information is such and such.

Let \(X\) - "Source" and \(Y\) = "Destination" be two random variables. 
There can be interpreted in many ways, here Source will be an external source having some variability (entropy) and Destination a collection of possible objects, such as types, actions, etc.

A first relation between these variables is mutual information 
\[
I(X;Y) = \sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)p(y)}
\]
where:
\begin{itemize}
    \item \(I(X;Y)\) is the mutual information between random variables \(X\) and \(Y\),
    \item \(p(x, y)\) is the joint probability distribution of \(X\) and \(Y\),
    \item \(p(x)\) and \(p(y)\) are the marginal probability distributions of \(X\) and \(Y\), respectively.
\end{itemize}
Mutual information quantifies the amount of information obtained about one random variable through the other.
This is somewhat analogous to correlation, but captures not only linear relationships and is proper for any collections


The vehicles of both randoms variables are phycisal objects (processes).
Usually it is assumed that the source generates a signal that is then received  at the destination.
The proper picture here is old telegraph


\end{frame}

 \begin{frame} [label=shannon_pipeline_source_channel] {Shannon pipeline with source and channel coding separation}

The Shannon pipeline with source and channel encoding / decoding parts: communication model 

\bigskip

    \begin{tikzpicture}[
    >=Latex,
    node distance=0.6cm,
    block/.style={draw, rounded corners, minimum width=2cm, minimum height=1.2cm, align=center}
]
% Blocks
\node[block] (srcenc) {Source\\Encoder};
\node[block, right=of srcenc] (chenc) {Channel\\Encoder};
\node[block, right=of chenc] (chan) {Channel\\$p(y\,|\,x)$};
\node[block, right=of chan] (chdec) {Channel\\Decoder};
\node[block, right=of chdec] (srcdec) {Source\\Decoder};

% Left input and right output
\coordinate[left=1.8cm of srcenc] (vin);
\coordinate[right=1.8cm of srcdec] (vhat);

% Arrows
\draw[->] (vin) -- (srcenc) node[midway, above] {$V^n$};
\draw[->] (srcenc) -- (chenc);
\draw[->] (chenc) -- (chan) node[midway, above] {$X^{n}(V^{n})$};
\draw[->] (chan) -- (chdec) node[midway, above] {$Y^{n}$};
\draw[->] (chdec) -- (srcdec);
\draw[->] (srcdec) -- (vhat) node[midway, above] {$\hat V^{\,n}$};

\node[block, above=of chan] (noise) {Noise $N$};
\draw[->] (noise) -- (chan);



\end{tikzpicture}

\brnote{\cite{cover_elements_2006}}

 \end{frame}   

 


\begin{frame}[label=gap_intro]
    There is a long-debated, deeply entrenched gap in the foundations of the project of naturalization of semantics and mental content: a gap between so-called semantic information and Shannon information (\cite{godfrey-smith_biological_2016,piccinini_information_2011}).

     \begin{quote}
        (...) three notions of information: Shannon information (information according to Shannon’s  communication theory), natural information (truth-entailing semantic information), and  nonnatural information (non-truth-entailing semantic information). \cite{piccinini_information_2011}
    \end{quote}

    \medskip 
    
      
    The gap has had a wide influence in cognitive science since the first applications of information theory to neuroscience (\cite{mackay_limiting_1952}) and in epistemology since Dretske (\cite{dretske_knowledge_1981}). Recognize two separate kinds of information: Shannon information and semantic information. 

   
   

  

\end{frame}




    \begin{frame}{Examples and Characterisations of Pipelines }

        -variables
        -Shannoon model of noisy communication 
        - algea phototaxix pipelines
        - jekely IO/IC table
        - vervet monkeys (compression according to Martinez)
        - Convolutional Neural Nets (Japan theorist Le Cun et al.)


    \end{frame}
    



\begin{frame}[label=sponge_larva]{ Cognitive(?) casusal processing pipeline in sponge larva}



    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[>=stealth, node distance=12mm, every node/.style={align=center}]
        % styles
        \tikzstyle{block} = [draw, minimum width=28mm, minimum height=12mm, rounded corners=1pt]
        \tikzstyle{thinblock} = [draw, minimum width=24mm, minimum height=12mm, rounded corners=1pt]
        \tikzstyle{labelnode} = [inner sep=0pt]

        % nodes
        \node (source) {\includegraphics[width=2cm,height=2cm]{images/fly03.png}};
        \node[labelnode, right=12mm of source] (msg) {$W$\\\small photons ordered collection};

    \node[thinblock, right=12mm of msg] (enc) {Retina};
    \node[labelnode, right=8mm of enc] (xn) {$X^n$};

    \node[thinblock, right=8mm of xn] (chan) {Channel / Brain / black box \\$p(y|x)$ };
    \node[labelnode, right=8mm of chan] (yn) {$Y^n$};

    \node[thinblock, right=8mm of yn] (dec) {Motor system};
    \node[labelnode, right=8mm of dec] (est) {${A}$\\\small (selected) action};

    % underbrace
    % Draw underbrace
    \draw [decorate,decoration={brace,mirror,amplitude=28pt}]
        (enc.south west) -- (dec.south east)
        node[midway,yshift=-34pt]{agent};
        
     

        \node[below=22mm of chan] (action) {\includegraphics[width=4cm,height=4cm]{images/frog03.png}};



        % arrows
        \draw[->] (source) -- (msg);
        \draw[->] (msg) -- (enc);
        \draw[->] (enc) -- (xn);
        \draw[->] (xn) -- (chan);
        \draw[->] (chan) -- (yn);
        \draw[->] (yn) -- (dec);
        \draw[->] (dec) -- (est);

    \end{tikzpicture}
    }

    \begin{center}
    I/O function is contained within a single cell or independent effector. Photosensitive ciliated cells in sponge larvae as examples of a single cell exhibiting input-output (IO) characteristics. Here,cells are photosensitive, they receive light information as sensory input. They use light-controlled rudder-like cilia to achieve phototactic steering. This means the light input directly influences the motor output (ciliary movement) of the same cell.
This function occurs without a need for nervous control!

From an efficiency standpoint, this mechanism is described as less efficient than a nervous system because "every motor component needs its own sensor," and "sensory mechanisms influence the activity of cilia on the same cell, thereby steering the whole larva". \autocite{jekelyOptionSpaceEarly2015}
 
    \end{center}



\end{frame}


\begin{frame}[label=basic_topologies]{Basic Topological Solutions}


\begin{tikzpicture}[node distance=2cm,>=stealth,thick]

\tikzset{dot/.style={circle,fill,inner sep=2pt}}

% 1. Parallel pipelines
\node[dot] (a1) {};
\node[dot] (b1) [right=of a1] {};
\draw[->] (a1) -- (b1);

\node[dot] (a2) [below=0.7cm of a1] {};
\node[dot] (b2) [right=of a2] {};
\draw[->] (a2) -- (b2);

\node[dot] (a3) [below=0.7cm of a2] {};
\node[dot] (b3) [right=of a3] {};
\draw[->] (a3) -- (b3);

\node[left=0.5cm of a2] {1.};

% 2. One-to-many (fan-out)
\node[dot] (c0) [below=2cm of a3] {};
\node[dot] (c1) [right=1.5cm of c0] {}; % junction
\node[dot] (c2) [right=2cm of c1,yshift=0.7cm] {};
\node[dot] (c3) [right=2cm of c1] {};
\node[dot] (c4) [right=2cm of c1,yshift=-0.7cm] {};
\draw[->] (c0) -- (c1);
\draw[->] (c1) -- (c2);
\draw[->] (c1) -- (c3);
\draw[->] (c1) -- (c4);

\node[left=0.5cm of c0] {2.};

% 3. Many-to-one (fan-in)
\node[dot] (d1) [below=2cm of c3, yshift=0.7cm] {};
\node[dot] (d2) [below=0.7cm of d1] {};
\node[dot] (d3) [below=0.7cm of d2] {};
\node[dot] (d4) [right=2cm of d2] {}; % junction
\node[dot] (d5) [right=1.5cm of d4] {}; % output
\draw[->] (d1) -- (d4);
\draw[->] (d2) -- (d4);
\draw[->] (d3) -- (d4);
\draw[->] (d4) -- (d5);

\node[left=0.5cm of d2] {3.};

\end{tikzpicture}


 \end{frame}



\begin{frame}[label=efficiency_role]{Efficiency Role: Neural Hypothesis}

    \begin{itemize}
        \item 
        
 In some non-neural organisms, like sponge larvae, sensory mechanisms directly influence the activity of cilia on the same cell to control steering. Inefficient because every motor component (like a cilium) requires its own sensor.


 The advent of neurons made it possible for a small number of sensory cells to control a large bank of motor devices, thereby increasing efficiency through division of labor and economies of scale.
•The addition of neurons  allows a few sensors to control many effectors. XXX Jekely cites someone
        
      
    \end{itemize}

   

\end{frame}





\begin{frame}[label=fly_frog_pipeline]{ Cognitive casusal processing pipeline of the agent in the environment}



    \resizebox{\textwidth}{!}{%
    \begin{tikzpicture}[>=stealth, node distance=12mm, every node/.style={align=center}]
        % styles
        \tikzstyle{block} = [draw, minimum width=28mm, minimum height=12mm, rounded corners=1pt]
        \tikzstyle{thinblock} = [draw, minimum width=24mm, minimum height=12mm, rounded corners=1pt]
        \tikzstyle{labelnode} = [inner sep=0pt]

        % nodes
        \node (source) {\includegraphics[width=2cm,height=2cm]{images/fly03.png}};
        \node[labelnode, right=12mm of source] (msg) {$W$\\\small photons \\ ordered \\collection};

    \node[thinblock, right=12mm of msg] (enc) {Retina};
    \node[labelnode, right=8mm of enc] (xn) {$X^n$};

    \node[thinblock, right=8mm of xn] (chan) {Channel / Brain / black box \\$p(y|x)$ };
    \node[labelnode, right=8mm of chan] (yn) {$Y^n$};

    \node[thinblock, right=8mm of yn] (dec) {Motor system};
    \node[labelnode, right=8mm of dec] (est) {${A}$\\\small (selected) action};

    % underbrace
    % Draw underbrace
    \draw [decorate,decoration={brace,mirror,amplitude=28pt}]
        (enc.south west) -- (dec.south east)
        node[midway,yshift=-34pt]{agent};
        
     

        \node[below=22mm of chan] (action) {\includegraphics[width=4cm,height=4cm]{images/frog03.png}};



        % arrows
        \draw[->] (source) -- (msg);
        \draw[->] (msg) -- (enc);
        \draw[->] (enc) -- (xn);
        \draw[->] (xn) -- (chan);
        \draw[->] (chan) -- (yn);
        \draw[->] (yn) -- (dec);
        \draw[->] (dec) -- (est);

    \end{tikzpicture}
    }

    \begin{center}
    Each part in the agent is performing some task function in the teleological sense. The inner part of pipeline has a dual characteristic "pushme-pullyou" of mental representation (\cite{millikan_pushme-pullyou_2017}).
    (characteristics)
    \end{center}



\end{frame}



\begin{frame}[label=examples]{Examples and Characterisations}

  begin{center}

  

Natural information ()

A nonnatural information is similar to mental representaion: "The tree in front of me". It can be false or inaccurate


    example 1: nonnatural, linguistic inforamtion : "Shop is OPEN", perhaps derived from mental representations. Can misrepresent 
 end{center}



\end{frame}

\begin{frame}[label=gap2]
    The gap is a symptom of a bigger theoretical explosion, which is not only a problem for cognitive science, but also for philosophy of mind and philosophy of language. 

    The goal of this presentation is to show that there is only one kind of information, and that the gap can be bridged by a suitable mechanism, which I will call "Shannon pipeline".

THIS I STHE PICTURE OF THE THEORIRESS OF INFORMATION 
(pieczarki)

    This gap is  one of the big class 


Just mention problems with particular solutions above.

    it is especially important for us because it is hoped that it can stand for the representation ; esp. structural representation
    which has some crucial properties for reps:
        -have aboutness 
        - can misrepresent
        -is drirected to its target (not symmetric)
        -has consumers that act upon them

show simmilarities

Floridi truthful-information (cant misrepresent?)
NONSEMANTIC INFORAMTION (PICCININIC)
FUNCTIONAL INFORAMTION  (FRESCO)

    FIlling it shows also how to get rid of other and finally stop inventing nek kinds of information and new kinds of its theories


\end{frame}




\begin{frame}[label=shannon_entropy]{Shannon Entropy =  Shannon Variability}

    1. There is no such thing as Shannon information. There is Shannon model of comminication.
    What does it do? It function task is rebiable transfer of information (not Shannon information)

    2.  Shannon entropy is quite often inaccurately described as uncertainty, knowledge, lack of knowledge, or disorder. The long stroy short: it is related with the fact, that entropy estimates the number of questions (binary, yes / no) that we need to ask to get the answer about the state of the world, given a distribution of objects in the world, to identify the state. That is however a psychlogical interpretation of Shannon entropy, which is not necessary for the inforamtion theory to work.

    \begin{itemize}
        \item The Shannon entropy, expressed as $- \sum p_i \log p_i$, is fundamentally understood as measuring the variability of the elements within a given distribution. 
        \item It characterizes the degree of diversity or how much variation can be found within a collection of objects.
        \item This interpretation aligns with the view that entropy measures variability, as emphasized by Carcassi (\cite{carcassi_variability_2021}).
        \item Information, then, is a state of a collection (of \textbf{any} objects).
    \end{itemize}
\end{frame}









\begin{frame}[label=shannon_flow] {First (quntifiable) information flow model (Shannon)}
    "Information flow" - how does it work?
    How did first model of communication work?
    Ask in the new mechanistic way: what are the parts, what do they do and (teleological part) what is the task of the pipeline

    REPETITION CODE AND MAJORITY VOTE - functions of encoder and decoder 
    Function of the whole mechanism: reliable transmission of information (collection)
    Order preserving come from the construction.
    Lack of order preserving (FIFOin this case) during transmission  be equal 
    So the momment of creating the communicatioon model there is order preserving involved.
     
\end{frame}


\begin{frame}[label=cognitive_pipeline]{A suitable cognitive Shannon pipeline for cognitive sciences explanations}
    
  

    A cognitive pipeline suitable for cognitive science explanations should then to get its job done, after Gałdziejewski Miłkowski,  satisfy several criteria:
    
    \begin{itemize}
        \item \textbf{Non-circular interpretation:} Following Carcassi's variability interpretation, avoiding psychological notions of "uncertainty" or "surprise" that lead to explanatory circularity.
        
        \item \textbf{Biological plausibility:} The pipeline components should map onto identifiable neural or neuroendocrine mechanisms and processes.
        
       
        
        \item \textbf{Error tolerance:} Real biological systems operate in noisy environments and must maintain functionality despite imperfect transmission.
        
        \item \textbf{Compositional capability:} The pipeline should support operations that enable complex representations to be built from simpler components.
         
        \item \textbf{Structure preservation:} Beyond mere correlation, the pipeline must preserve relevant structural relationships from input to output.
    \end{itemize}
    
    \textbf{Key insight:} The standard Shannon model provides the foundation, but requires extensions (like the proposed noisy addition) to bridge the gap to semantic information processing in cognitive systems.

    
\end{frame}



\begin{frame}[label=pipeline_structure]

    HOW SHOULD IT LOOK LIKE 
    source: real external or internal variabilites (can conatin "real patterns")
    the encoder - some kind of processing the input variability
    the channel - any influence from external or internal environment (a black box, really)
    the decoder - some kind of processing the input variability "down the stream"
    the receiver - can be counted as part of pipeline or not

    Examples (sunlight, alge, movements)
    Vervets signalling

    This is also an implementation model of pushme-pullyou of the representation posited by Millikan
    (\cite{millikan_pushme-pullyou_2017}).

    What is the algea doing in this 2 seconds after sun? 
    
\end{frame}




\begin{frame}[label=coins_setup] {COins in the pocket setup}
    \begin{center}
        Correlations can be perfectly accidental and still carry natural information
        Pipelines can have multiple sources.
        Pipeleines can be branched.
        
        Draw the example of pipelines in case of the "Coins in the pocket setup"

        
        This kind of setup is multiplied and  combined in social networks, which explains Dennets Trafalgar square exaple: it shows "the channel" of his interest as branches structed of causally connected elements, sources, senders, receivers.

    \end{center}
\end{frame}



\begin{frame}[label=early_neural]{Early neural systems as information pipelenes}

 Jekely stresses out hybrid IO + IC pipeleines (\cite{jekely_neural_2013}): 
    "The neural system of the cnidarian Hydra is a simple, yet functional, nervous system that can be described as a hybrid input-output and input-computation system. It is composed of a network of neurons that can process and transmit information, allowing for complex behaviors such as swimming and feeding." - XXX Check this

    DOdać tablekę z artykułu

    \bigskip
    
    \textbf{Neurons in the pipeline:}

When neurons appear in the evolution is unclear, bu given the Jekely charcterization they 
have "direction" either manytoone one to many 

Both solutions can be useful
show how (entropy in classification)
branching in steering 

both pipelines can be achieved from mere one-dimensional copiers (like Shannon communication model)

\end{frame}




\begin{frame}[label=claims]{My claims for today}
    
    \begin{center}
        
        
        \begin{enumerate}
            \item Applications of Shannon's information theory and communication model seem not only to be relevant (\cite{Mann2023}) but also useful (\cite{martinez_representations_2019}) for the naturalistic project of explaining mental content (mental representations).  
           
            \item Yet there is, I think, some gap between information theory capabilities and representation theory needs, only partially addressed by philosophers: the gap related to the compositionality of content. 
            \item If there is a gap, then the goal is to bridge it \textbf{without positing a new kind of information}  and \textbf{show that there is only one kind, though mechanisms processing information can differ enormously}. 
        
            \item I also propose a mechanism that seems to show the way to  bridge the gap.
                      
        \end{enumerate}
    \end{center}
\end{frame}


\begin{frame}[label=info_theory_reps] {Representations in the information theory: a way that I won't choose today}
        
    Small disclaimer. 
    Originally, Dretske required the necessity of conditional probabilitity for representation (\cite{Dretske1981}), which is unrealistic in a noisy world: representations often misrepresent. On the other hand, simply raising the conditional probability of a stimulus given a representation is too weak to establish a representational relation between random variables $X_i$ and $Y_j$: it brings indeterminacy of content as immediate problem.   Additionally, mutual information $I(X_i;Y_j)$ is symmetric. \textbf{These consequences seem a bit unrealistic. Hint: original Dretske's setup lacks crucial parts of the Shannon's pipeline.}          
            \begin{tikzpicture}[node distance=2cm, thick]
            
                % Node Styles
                \tikzset{circle_node/.style={circle, fill=lightgray, minimum size=0.6cm}}
                \tikzset{ellipse_node/.style={ellipse, fill=yellow, minimum height=3cm, minimum width=3cm, text centered, draw}}
                \tikzset{diamond_node/.style={diamond, fill=gray!70, minimum size=0.8cm}}
                
                % Nodes
                \node[circle_node] (A) {$X_1$}; % Circle node on the left with $X_1$
                \node[diamond_node, above left=1cm and 1cm of A] (B) {$X_2$}; % Diamond node below left with $X_2$
                \node[ellipse_node, right=3.5cm of A] (Y) {$Y_i$,  $Y_j$,   $Y_k$}; % Ellipse node on the right with $Y_i$, $Y_j$, $Y_k
                
                % Images
                \node [left= 0.5cm of B]{\includegraphics[width=2cm]{images/mucha.jpg}};
                \node [left= 0.5cm of A]{\includegraphics[width=2cm]{images/kropki.jpg}};
                \node [right= 0.5cm of Y]{\includegraphics[width=3cm]{images/zaba.jpg}};
            
                % Titles
            
                
                % Arrows and connections
                \draw[thick,->] (A) -- (Y); % Arrow from left circle to ellipse
                \draw[->, thick] (B) -- (Y); % Arrow from diamond to ellipse
                
            \end{tikzpicture}
           
            

    \end{frame}


\begin{frame}[label=kinds_vehicles]
    \begin{center}
      {\Huge  Kinds of information and their vehicles } % Use \Huge for very large font size
    \end{center}
\end{frame}


\begin{frame}[label=semantic_meaning] {What does "semantic" mean, roughly}
    \begin{center}
        Sematnicity is a property of representations (words, images, maps) that are about something, that have "aboutness" or "intentionality" (in the sense of Brentano, \cite{brentano_psychology_1995}) - but in a distinguished way.

        Tarski (who inspired Carnap invastigation of language meaning conceived as sets of functions from possible worlds to truth values)
        Tarski defined truth using
        -  terms that refer to domain 
        - logic constants (e.g. "and", "or", "not"), quantifiers
        - recursive rules of constructing thet properly build formulas such that what they refer to is a function of the referntial terms and logical constants.
        
        That kind of buolding sets of languae representatoins of a domain; which are systematically related : their values depend only on the values of the referential terms and logical constants used in them.


    \end{center}
\end{frame}




\begin{frame}[label=semantic_vehicles]{Semantic information and its vehicles}
 
We know well many examples of vehicles of semantic information:
 
Obvious cases:
\begin{itemize}
    \item true sentences (e.g. "The cat is on the mat" refering to the state of the world)
    \item maps of domains (e.g. terrain).
    \item photos and pictures, paintings.
\end{itemize}

Controvertial cases:
\begin{itemize}
    \item false sentences
    \item distorted images
    \item empty descriptions (e.g. "the king of France")
   
\end{itemize}

\textbf{\color{red} Three important comments here:}
\begin{itemize}
    \item All these cases "derive" or "inherit" their semantic information from mental content ("non-natural information"). 
   \item For sake of simplicity, I will equate them with structural representations (below)
    \item They are not only structural, but in most cases compositional.
\end{itemize}

 \end{frame}


 \begin{frame}[label=structural_conditions]{Conditions on structural representaions and its vehicles}

    Structural representations should meet the job description (\cite{ramsey_representation_2007}) for representations (\cite{gladziejewski_explaining_2015-1,gladziejewski_structural_2017}). 
 
     \begin{itemize}
         \item \textbf{Structural Similarity Condition:} The component part of the mechanism (the representational vehicle) should have a structural resemblance to the object it represents. 
         Important: misrepresetation is always  possible, because similarity comes in degrees.
         
         \item \textbf{Action-Guidance Condition:} The representation must guide the actions of the larger cognitive system.
         
         \item \textbf{Decouplability Condition:} The representation should be able to function even when the object it represents is not present.
         
         \item \textbf{Error-Detection Condition:} The system using the representation must have the ability to detect when the representation is inaccurate or false. 
     \end{itemize}

 
 \end{frame}



\begin{frame}[label=similarity]{Similarity can be well defined }
    
    \begin{itemize}
        \item Similarity is a crucial aspect of representation, ensuring that the essential relationships and properties of the represented domain are maintained in the representing domain. 
        
        \item Similarity can be defined with the concept of homomorphism, which is a mapping that preserves the structure of set of objects and its map. Another way is defining similarity by abstract distance between some objects (e.g. strings "000000" and "00010" are quite similar, but not equal). This kind of similarity can be measured and is very natural in Shannon's theory toolkit.
       
        \item In the context of representation, homomorphisms ensure that the representing domain accurately reflects the structure of the represented domain, allowing for  inferences and predictions. 
    
  
    \end{itemize}
    
\end{frame}


\begin{frame}[label=shannon_vehicles]{Shannon information and its vehicles}

   The interesting cases of the vehicles of Shannon information are:
    \begin{itemize}
       
        \item binary strings (e.g. sequences of bits)
        \item words (e.g. in a newspaper)
        \item DNA sequences 
        \item waggle dance of honeybees
        \item neural spike trains (or some other aspects of bioelectric activity of neurons)
       
        \item ..any collections of any objects have information-theory properties such as entropy,and mutual information with some (other) collection. 
        
        
    \end{itemize}

\end{frame}


\begin{frame}[label=shannon_intro] { Shannon information and its vehicles (a very short intro) }
   
   Since Hartley (\cite{hartley_transmission_1928})  we can think about information as a non-psychological, quantifiable aspect of groups of objects, that could be carried or stored or transmitted or processed whatever these objects are. 

Shannon  extended Hartley’s approach, and introduced a statistical concept of entropy to measure this property. To compute the entropy of  the collection, we need only to know the distribution of it, that is the probabilities of elements in the collection.



\end{frame}


\begin{frame}[label=shannon_intro_cont] { Shannon informtion and its vehicles (a very short intro) cont. }
The entropy of $X$ is defined as:
$$
H(X) = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
$$

where:
\begin{itemize}
    \item $H(X)$ is the entropy of the random variable $X$.
    \item $\mathcal{X}$ is the set of all possible values of $X$.
    \item $x$ is a specific value of $X$.
    \item $p(x)$ is the probability of $X$ taking the value $x$.
\end{itemize}

\textbf{"It measures the variability of the elements within a given distribution, giving it a crisp intuitive meaning that is general and applicable to all branches of science"} \cite{carcassi_variability_2021} 

The amount of Shannon information is then a statistical property of its vehicles. 
  
\end{frame}



\begin{frame}[label=gap_suspect]
    \begin{center}
      {\Huge 2. The Gap. Why suspect there is  more than one kind of information?} % Use \Huge for very large font size
    \end{center}

    \bigskip
    
    There is an abundance of arguments for the existence of more than one kind of information, here I will recall three of them.


\end{frame}

\begin{frame}[label=levels_leibniz] {Levels of Organisation / Leibniz's mill argument} 
   
    \begin{itemize}
    
        
      \item  Rathkopf calls this state of affairs a bifurcation: "At a high level of neural organization, brain information is semantic, but down at the level of single neurons, semantic properties are irrelevant, and the only information to speak of is Shannon information."  (\cite{rathkopf_what_2020})
      
     
    \end{itemize}
      
  
  \end{frame}

\begin{frame}[label=trafalgar]{Dennett's Trafalgar Square Argument}
    
    Dennett provides a toy model to illustrate the gap (\cite{dennett_bacteria_2017}): 
    
    "Jacques shoots his uncle dead in Trafalgar Square and is apprehended on the spot by Sherlock. Tom reads about it in the Guardian, and Boris learns of it in Pravda.\textbf{ Jacques, Sherlock, Tom, and Boris have had remarkably different experiences, but they share one thing: semantic information that a Frenchman has committed a murder in Trafalgar Square.} (...) They share no encoding, but they do share semantic information."

    \bigskip
    
    \begin{quote}
    "Semantic information, the concept of information that we must start with, is remarkably independent of encodings, in the following sense: two or more observers can acquire the same semantic information from encounters that share no channel" ()\cite{dennett_bacteria_2017}). 
    \end{quote}

    I will argue below that "seeing" these channels is a ambitious, open scientific project.

\end{frame}



\begin{frame}{Taking information for its measure }

    It is quite common in the literature to identify information with information entropy.
    (Piccinini)

    

\end{frame}


\begin{frame}[label=equal_entropy]{Argument from Equal Entropy}
  Two different distributions can have the same Shannon entropy, $H(X)$, even if their internal structures differ significantly. 

\begin{center}
    \includegraphics[width=0.25\textwidth]{images/entropia_1.png}
    \includegraphics[width=0.25\textwidth]{images/entropia_2.png}
\end{center}
   The first receptive field may represent tasty food that some  fish could eat. Second is rather only a dirt! So the information first differs from the information in second one.
\end{frame}

\begin{frame}[label=equal_entropy_cont]{Argument from Equal Entropy (cont.)}    
       
This argument is flawed in at least two respects.

First, whats is equal, is only the measure (entropy). Cf. a glass of water can carry the same amout of water as a bottle.    Shannon entropy captures the variability or uncertainty of a distribution but \textbf{does not capture the specific arrangement or structure of the data}.


    So the collorary is mistaken: it is not the case that there is a new kind of information, but rather that we should look for mechanism preserving order  or internal structure.

    E.g. in retina this order may be preserved just by spatial encoding that is then encoded by time relations between spikes

   


\end{frame}




\begin{frame}[label=core_info]{The core of information theory}
Shannon information theory is more than Dretske  first used when starting his program of naturalizing knowledge.  
    \textbf{Crucially, it posits a mechanism called famously "Shannon communication model".}  This model has a task function.

“The fundamental problem of communication is that of reproducing
at one point either exactly or approximately a message selected at
another point.” (\cite{shannon_mathematical_1948})
\textbf{
Shannon model's task function, we mights say, is  optimizing reliabilty of transfer. What ELSE it is able to perform, is an open question}
    


\end{frame}




\begin{frame}[label=new_mechanists]{Towards New Mechanists interpretaion of information theory models}

Biology and neuroscience explain phenomena by uncovering mechanism. Mechanism” is  an organized system of components and activities such that the components and activities, organized the way they are, produce the phenomenon (Bechtel and Richardson 1993; Glennan 1996; Machamer, Darden, and Craver 2000).

Information theory has the communication model in its ontological commitments. When applied, the model equates to a kind of mechanism or is a part of bigger mechanism (in the sense of New Mechanists, \cite{machamer_thinking_2000,glennan_rethinking_2002}).
    
   \emph{ Probably the best way to think about this mechanism is in terms of "information processing pipeline" (\cite{martinez_representations_2019}).}

    Two of its 5 parts (encoder and decoder) are allowed to process information in any way that improves reliability of transmission of information.

    Importantly, this is a abstract mechanism description that we can map on many biological phenomena, organs, parts of organisms etc. But this practce is problematic (Nizami \cite{nizami_information_2019}).

    3 main theorems in the core of the information theory  relate to  different parts of the model: these are source coding theorem, channel coding theorem, and rate distortion theorem. They describe COMPRESSION  RATE LIMITs (resp. lossless transmission, lossy compression, and trade off between rate and distortion) of the communication model.
\end{frame}


\begin{frame}[label=shannon_model]{ Shannon Model (\cite{shannon_mathematical_1948})}

    \includegraphics[width=1\linewidth]{images/latexImage_9d58f9e54772e4e91104b2b791c5c63e.png}
    
    \begin{tikzpicture}[remember picture, overlay]
        % Example: Placing a text node at a specific position over the imagethey
        \node at (1.5cm, 3cm) {\huge \textbf{\textcolor{red}{X} }};
        \node at (12.5cm, 3cm) {\huge \textbf{\textcolor{red}{$\hat{X}$}}};
        \node at (5cm, 3cm) {\huge \textbf{\textcolor{red}{encoder}}};
         \node at (9cm, 3cm) {\huge \textbf{\textcolor{red}{decoder}}};
    \end{tikzpicture}


Encoder and decoder are  machines performing some algorithms  in the given implementation.
The elements of the pipeline are then in \textbf{causal relations.} ("Cheap correlations" allegations can be then dismissed, if we are sure about the causal relations between the elements of the pipeline - if they are established independently)

    The whole pipepeline is also a machine, a stochastic one, when a channel is noisy.
    {$\hat{X}$} predicts or guesses the value of $X$.
   

\end{frame}


\begin{frame}[label=shannon_mechanism]{Shannon model as mechanism}
 
    The classic Shannon communication model  is a mechanism in this sense.
    According to the New Mechanists:

    Mechanisms are entities and activities organized in such a way that they are
    productive of regular changes from start or set-up to finish or termination conditions.
    (...)
    Mechanisms are composed of both entities (with their properties) and
    activities. Activities are the producers of change. Entities are the things
    that engage in activities. (\cite{machamer_thinking_2000})

    
    "A mechanism underpinning a behavior is a complex system that produces
    that behavior by the interaction of a number of parts, where the
    interactions between parts can be characterized by direct, invariant,
    change-relating generalizations." (\cite{glennan_rethinking_2002})

   

\end{frame}


\begin{frame}[label=shannon_pipeline]{Shannon pipeline basic scheme}
 
   
    \subsection*{Shannon Communication Pipelien with Noisy Channel as Mechanistic Component}
        \begin{center}
        \begin{tikzpicture}[node distance=2cm, thick, every node/.style={font=\small}]
            \node (source) [draw, minimum width=2.5cm, minimum height=1cm] {Source $S$};
            \node (encoder) [draw, rectangle, right of=source, node distance=3.2cm, minimum width=2.5cm, minimum height=1cm] {Encoder};
            \node (channel) [draw, rectangle, right of=encoder, node distance=3.2cm, minimum width=3cm, minimum height=1.5cm, fill=red!10] {Channel $P(y|x)$};
            \node (decoder) [draw, rectangle, right of=channel, node distance=3.2cm, minimum width=2.5cm, minimum height=1cm] {Decoder};
            \node (sink) [draw, right of=decoder, node distance=3.2cm, minimum width=2.5cm, minimum height=1cm] {Destination $E$};
            \node (noise) [draw, rectangle, above of=channel, node distance=2cm, minimum width=2.5cm, minimum height=1cm, fill=blue!10] {Noise $N$};
            \draw[->] (source) -- (encoder);
            \draw[->] (encoder) -- (channel);
            \draw[->] (channel) -- (decoder);
            \draw[->] (decoder) -- (sink);
            \draw[->] (noise) -- (channel);
        \end{tikzpicture}
        \end{center}

    \end{frame}


  
 \begin{frame}[label=shannon_noisy]{Shannon Communication Pipeline with Noisy Channel as Mechanistic Component}
         
           

                \begin{center}
                \begin{tikzpicture}[node distance=2cm, thick, every node/.style={font=\small}]
                    \node (source) [draw, minimum width=2.5cm, minimum height=1cm] {Source $S$};
                    \node (encoder) [draw, rectangle, right of=source, node distance=3.2cm, minimum width=2.5cm, minimum height=1cm] {Encoder};
                    \node (channel) [draw, rectangle, right of=encoder, node distance=3.2cm, minimum width=3cm, minimum height=1.5cm, fill=red!10] {Channel $P(y|x)$};
                    \node (decoder) [draw, rectangle, right of=channel, node distance=3.2cm, minimum width=2.5cm, minimum height=1cm] {Decoder};
                    \node (sink) [draw, right of=decoder, node distance=3.2cm, minimum width=2.5cm, minimum height=1cm] {Destination $E$};
                    \node (noise) [draw, rectangle, above of=channel, node distance=2cm, minimum width=2.5cm, minimum height=1cm, fill=blue!10] {Noise $N$};
                    \draw[->] (source) -- (encoder);
                    \draw[->] (encoder) -- (channel);
                    \draw[->] (channel) -- (decoder);
                    \draw[->] (decoder) -- (sink);
                    \draw[->] (noise) -- (channel);

                    \draw[->, red, thick] (encoder) -- node[midway, above, sloped, text=red] {causal relation} (channel);

                    
                \end{tikzpicture}
                \end{center}
             
                \vspace{0.5cm}
                \textbf{Properties of pipeline (not the variability being processed)}
                \begin{itemize}
                    \item \textbf{Task Function:} Reliable transmission or storage of information from point to point Source to Destination. Intended a real world device - mechanism.
                    \item \textbf{Compositionalty type /ORDER TYPE:} FIFO queue; not  compositional, FIFO IS a kind of order!!! rozwiń to w slajdzie 
                    
                    \item \textbf{Encoder}: Increases redundancy of the information before sending as "signal" through the Cchannel.
                    \item \textbf{Decoder }: Reduces redundancy, reconstructs the original message given the signal received from the channel.
                    \item \textbf{is Semantic:} Yes, grounded in the accuracy conditions (Source).
                    \item \textbf{is Deterministic:} No, due to the presence of noise in the Channel.
                    \item 
                    \item \textbf{Example:} , such as a biological neural system.
                \end{itemize}

                \vspace{0.5cm}
                \textbf{Commentary:} The Shannon pipeline is a foundational model for understanding information transfer. While it is not inherently compositional, its extensions can support more complex representational tasks.
        \end{frame}







\begin{frame}[label=semantic_shannon]{ Semantic properties of Shannon pipeline }

        \begin{figure}[h!]
            \centering
            \includegraphics[width=1\linewidth]{images/Dilbert.png}
            \caption{Image transmitted through noisy channel, probability of bit flip = $f$ (\cite{MacKay2003})}
        \end{figure}
    
        \begin{tikzpicture}[remember picture, overlay]     
            \node at (4cm, 6cm) {\textbf{\textcolor{red}{accuracy condition}}};
             \node at (11cm, 6cm) {\textbf{\textcolor{red}{ semantic information}}};
        \end{tikzpicture}

        Accuracy conndition is the Source in the pipeline.
        Shannon model, when applied to cognitive subsystem of an organism, has accuracy conditions; that accuracy can be measured by mutual information between the source and the destination. Thus, the pipepile  is semantic: it is grounded in the Harnad sense (\cite{harnad_symbol_1990})

\end{frame}



\begin{frame}[label=structural_candidate]
    \begin{center}
      \Huge A candidate for structural representation
    \end{center}
\end{frame}






\begin{frame}[label=shannon_structural]{Why Shannon model is a good candidate for a structural representaton?}
    \begin{itemize}
        \item \textbf{Structural Similarity Condition - } The Shannon model inherently preserves the {\color{red} probabilistic structure} of the source and the transmitted signal, ensuring a degree of resemblance between the input and output.
        \item \textbf{Action-Guidance Condition - } The encoded and transmitted information carried by parts-vehicles of the model can guide actions in systems that  utilize the signal, such as neural networks.
        \item \textbf{Decouplability Condition -} The Shannon model allows for the transmission or storage of information independently of the presence of  source, enabling decoupled representations (hallucinations are possible and can be common). 
        \item \textbf{Error-Detection Condition -} The original Shannon model implements mechanism for error correction by design.
    \end{itemize}

    {\color{red}  Notice: probabilistic structure is not the same as rigid structure in the sense of compositionality. This is I thnik  one of the reasons for imperfect inferetial capabilities of large language models. }
    
\end{frame}




\begin{frame}[label=shannon_structural_cont]{Why Shannon model is a good candidate for structural representation (cont.)?}
    \begin{itemize}
        
        \item Compression  algorithms can be used to reduce the amount of information transmitted while preserving the essential structure of the data. (information processing efficiency implies less energy spending)
        
        \bigskip
\textbf{        {\color{red} Below I claim more }
}     
        \item \textbf{Compositionality.} The model can be extended to handle \textbf{noisy addition}, allowing for the representation of complex structures and relationships, which is essential for compositionality of representations.
       
       \textbf{ This condition, I think, closes the gap between semantic information and Shannon informationl}
    \end{itemize}

   
\end{frame}



\begin{frame}[label=shannon_assumptions]{ Probabilistic Assumptions of Shannon Model \\ (\cite{shannon_mathematical_1948})}

    Multiple use of channel is assumed with no relation between elements of the sequence transmitted (for generality of Shannon theorems). Real world sources produce ordered signals.

\textbf{    1. Properties of source sequences : indendence of elements in sequence  }

$$
A_1, A_2, \ldots A_k \rightarrow \Gamma \rightarrow B_1, B_2, \ldots B_k
$$
independence of symbols:
$$
p\left(b_1, b_2, \ldots b_k \mid a_1, a_2 \ldots a_k\right)=p\left(b_1 \mid a_1\right) \cdot p\left(b_2 \mid a_2\right) \cdot \ldots \cdot p\left(b_k \mid a_k\right)
$$
no memory:
$$
p\left(b_k \mid a_1 \ldots a_k, b_1 \ldots b_{k-1}\right)=p\left(b_k \mid a_k\right)
$$
no feedback:
$$
p\left(a_k \mid a_1 \ldots a_{k-1}, b_1 \ldots b_{k-1}\right)=p\left(a_k \mid a_1 \ldots a_{k-1}\right)
$$


(Niwiński, lectures)

\end{frame}



\begin{frame}[label=order_preserved] {Why order is  preserved on this conditions in the transmission at all?}

    This is simply a consequence of the construction of the channel!

    Shannon model processes the elements of sequences \textbf{"first in, first out"}, thus saving their order.

    So, if some order can be transferred, can there be homomorphisms achived?     And if we have hoomorphisms, we can have compositionality?

\end{frame}

\begin{frame}[label=shannon_majority]{ Shannon model with redundancy encoding and majority vote decoder}
    \textbf{   (Noisy) channel is defined as a matrix of conditional probabilities. }
    

    \begin{figure}[h!]
        \centering
        \begin{minipage}{0.6\textwidth}
            \includegraphics[width=\linewidth]{images/majority_vote.png}
            \caption{Majority vote rule}
        \end{minipage}
        \hfill
        \begin{minipage}{0.6\textwidth}
            \includegraphics[width=\linewidth]{images/improving.png}
            \caption{Reliability improving mechanisms}
        \end{minipage}
     
    \end{figure}
    
    (Niwiński, lectures)
    
    \end{frame}









  

    \begin{frame}[label=shannon_channel] {It is not an easy task to see the Shannon communication model }
    
        Now we may go back to Dennets argument for a while. Good mechanistic explanations should be able to map the elements of Shannon model on the cognitive pipelines. 
     
     The 3M requirement (\cite{kaplan_explanatory_2011-1} ) holds that for a model to count as a proper mechanistic explanation (as opposed to just a phenomenological description), it must satisfy two core conditions:
     
     The variables in the model must correspond to identifiable components, activities, and organizational features of the actual mechanism that produces, maintains, or underlies the phenomenon.
     
     The dependencies or relations posited among these variables in the model must correspond to causal relations among the components of the target mechanism
     
     But there is no consensus in neuroscience how to map them!   Cf.  Nizami's critique and his table from hell \cite{nizami_information_2019}
     He estimates the number of possible mappings of the Shannon model on the organisms for thousands. 
     
     
     \end{frame}

\begin{frame}[label=nizami_table]  {But how to map Shannon Model? Nizami's table} 
   
  Nizami provides a table of possible mappings of the Shannon model onto the environment and parts of organisms. He estimates the number of possible mappings found in the literature for milions of possibilities. This is an open scientific project.
        \begin{center}
            \includegraphics[width=1\textwidth]{images/Nizami_table1.png}
            \captionof{figure}{First page of Nizami's table of  mappings of the Shannon model in neuroscientific literature (\cite{nizami_information_2019}).}
        \end{center}

\end{frame}

\begin{frame}[label=nizami_table_cont]  {But how to map Shannon Model? Nizami's table cont.} 
    \begin{center}
        
       
            \includegraphics[width=1\textwidth]{images/Nizami_table2.png}
            \captionof{figure}{Last page of Nizami's table of  mappings of the Shannon model in neuroscientific literature (\cite{nizami_information_2019}).}
      


    \end{center}
\end{frame}







\begin{frame}[label=phototaxis]{Phototaxis representational mechanisms: \\ semantic but not compositional}
    \small
    Something more sophisticated than thermostats: phototaxis (in algae) occurs when whole organisms move toward a light source for photosynthesis.
    
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \includegraphics[width=\linewidth]{images/phototaxis.png}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Phototactic movements in \textit{V. rousseletii}:}
            \begin{itemize}
                \item  Straight-ahead swimming in the dark.
                \item  A sudden dark-light switch reverses flagellar beating in the anterior hemisphere, decelerating forward motion (photophobic response).
                \item  After 2 seconds, only the illuminated side's cells reverse flagellar beating, turning the spheroid toward the light.
            \end{itemize}
            \tiny \textit{Adapted from \cite{ueki_how_2010}}
        \end{column}
    \end{columns}
    
    \vspace{1em}
    
    We can measure mutual information between the "movement alphabet" \{\textit{stop, follow}\} and "receptive field" \{\textit{dark, dark-light, light-2s}\}. But this yields only a black-box type of IT-based explanation—the underlying encoder/decoder mechanisms remain unknown.\\
    
    This kind of phototaxis is not (even) a neural mechanism. 
    Compare XXX Jekely

    
    \end{frame}
    





    \begin{frame}[label=compositionality]{Many forms of compositionality}

"Compositionality is all around us " \autocite{coecke_mathematical_nodate}

Somewhat paradoxically, most natural domain for compositionality are artificial laaguages (after Frege). Natural language includes many counterexamples (e.g. "red herring")

\begin{quote}
The meaning of a compound expression is a function of the meaning of its parts and of the syntactic rule by which they are combined (\cite{janssen_compositionality_2012,janssen_frege_2001})
\end{quote} 
Compositionality is a general property of \textbf{complex} representational systems, but not only languages:

It is essential for linear algebra on multidimensional spaces:
\begin{itemize}
   
    \item \textbf{Distributivity of scalar multiplication with respect to vector addition:} For all scalars $c$ and all vectors $u$ and $v$ in $V$, $c(u + v) = cu + cv$.
    \item \textbf{Distributivity of scalar multiplication with respect to scalar addition:} For all scalars $c$ and $d$ and all vectors $v$ in $V$, $(c + d)v = cv + dv$.
\end{itemize} (\cite{axler_linear_2024})


We can also think of source sequences this way: we model world with linear algebra in science.
\textbf{\color{red} But that is not what Shannon model supports out of the box: it does not provide arithmetical operations on real numbers.}


\end{frame}



\begin{frame}[label=compositionality_table]{Many forms of compositionality}
   

\begin{table}[h!]
\centering
\small % Reduce font size
\begin{tabular}{|p{2cm}|p{4cm}|p{3cm}|p{3cm}|}
\hline
\textbf{Property} & \textbf{Definition \& Core Idea} & \textbf{Mathematical/Structural Formulation} & \textbf{Example Contexts} \\ \hline
Linearity in Algebra & A system is linear if it satisfies additivity and homogeneity: the output for a sum or scaled input equals the sum or scaled output (superposition principle). & $f(x + y) = f(x) + f(y)$, $f(\alpha x) = \alpha f(x)$ & Linear maps, vector spaces, circuits \\ \hline
Compositionality in Language & The meaning of a complex expression is determined by its structure and the meanings of its parts. & --- & Syntax, semantics, formal languages \\ \hline
Homomorphism & A structure-preserving map between two algebraic structures of the same type, preserving operations and relations. & $f(x \cdot y) = f(x) \cdot f(y)$ (for operation $\cdot$) & Groups, rings, vector spaces \\ \hline
\end{tabular}
\caption{Comparison of Properties in Algebra and Language}
\label{tab:properties}
\end{table}


    
\end{frame}


\begin{frame}[label=smolensky]{Compositionality "from" vector spaces - Smolensky}

    Once system can perform linear operations in multidementional spaces,system can reconstruct recursive syntax (\cite{smolensky_tensor_1990})
\small
    \begin{tabular}{|c|c|c|c|c|}
    \hline {\begin{tabular}{l} 
    Structuring \\
    operation
    \end{tabular}} & \multicolumn{2}{|l|}{ Symbolic formalization } & \multicolumn{2}{|l|}{ Vector embedding formalization } \\
    \hline & Structures & Example & Example & Vector operation \\
    \hline Combining & Sets & $\left\{\mathrm{c}_1, \mathrm{c}_2\right\}$ & $\mathbf{c}_1+\mathbf{c}_2$ & Vector sum + \\
    \hline \begin{tabular}{l} 
    Role/filler \\
    binding
    \end{tabular} & Strings, frames & $A B=\left\{r_1: A, r_2: B\right\}$ & $\mathbf{r}_1 \otimes \mathrm{~A}+\mathbf{r}_2 \otimes \mathrm{~B}$ & Tensor product $\otimes$ \\
    \hline \begin{tabular}{l} 
    Recursive \\
    embedding
    \end{tabular} & Tree & & \begin{tabular}{l}
    $\mathbf{r}_0 \otimes \mathrm{~A}$ \\
    + \\
    $\mathbf{r}_1 \otimes\left[\mathbf{r}_0 \otimes \mathbf{B}+\mathbf{r}_1 \otimes \mathbf{C}\right]$
    \end{tabular} & \begin{tabular}{l} 
    Recursive role \\
    embeddings: \\
    $\mathbf{r}_{\text {child }_{0 / 1}}(x)=\mathbf{r}_x \otimes \mathbf{r}_{0 / 1}$
    \end{tabular} \\
    \hline
    \end{tabular}
    
    Combinatorial neural representations: Tensor Product Representations 
    - The vector embedding the whole is the sum of the vectors embedding the constituent role/symbol bindings:
    $$
    a(\text { structure })=a\left(\mathrm{R}_1: \mathrm{S}_1\right)+a\left(\mathrm{R}_2: \mathrm{S}_2\right)+\cdots
    $$
   
    
    \end{frame}



\begin{frame}[label=cnn_representations]{S-representations seem to exist in convolutional neural networks}
\small
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \includegraphics[width=\linewidth]{images/cnn.jpg}
        \end{column}
        \begin{column}{0.5\textwidth}
            Structural representations (and metarepresentations) seem to exist in CNNs.
    
            \begin{itemize}
                \item \textbf{\color{red}Structural similarity} to the features present in training data (but homomorphisms can be hard to identify). Are they compositional? Not fully or not at all (constant problems with inferences in large language models).
                \item \textbf{\color{red}Veridical}: classify with accuracy  open classes of examples (mis)representation capabilities.
                \item \textbf{\color{red}Contentful}: clusters of vectors in weight matrices can represent Dennet's "real patterns" or natural kinds.
            \end{itemize}
        \end{column}
    \end{columns}
    
    \vspace{0.5em}
    \textbf{Can we express a structural representation by pure means of information theory?}
  
    
    \end{frame}
    

\begin{frame}[label=cnn_structure]
    \frametitle{How (convolutional) artificial neural preserve structure?}
    \textbf{They use numbers.}
    The smallest building blocks of most of the  networks (artificial neurons) use  linear algebra and compute sums of weighted inputs, which is a linear, compositional  operation.    
    
    \textbf{To fit them in the Shannon model picture let us assume that the incoming elements of a sequence are ordered} - as in binary code.  The sequences of objects from the alphabet set \(\{0,1\}\) in the input signal are then ordered from the most important to the least important (or from the strongest to the weakest). In this binary sequence, the first bit is the most important or the strongest, and the last bit is the least important or the weakest, with the \((n-1)\)-th element being two times weaker than the \(n\)-th element. 

For example, the sequence:
\[
a_{1..4} = 1011
\]
encodes a signal of intensity:
\[
2^3 + 2^1 + 2^0 = 11.
\]
    

\end{frame}


\begin{frame}[label=barlow]{Biological Plausibility: Barlow’s hypothesis}

    \begin{columns}[T] % The [T] option aligns the columns at the top
   
           % Column for the picture
           \begin{column}{0.5\textwidth}
               \includegraphics[width=1\linewidth]{images/latexImage_aad051c66a8ab8fe36b14843445faa7f.png} % Adjust the path and filename
           \end{column}
   
           % Column for the text
           \begin{column}{0.5\textwidth}
            Sensory neuron's firing rate reflects the strength of stimulation (Adrian, Zotterman, 1926), a higher rate indicating a stronger stimulation (but the size of each spike remains constant).
           \end{column}
   
       \end{columns}
   
   
   In 1953, Barlow hypothesized:

   \begin{itemize}
       \item \textbf{the stronger the stimuli, the richer are the spike trains} (interpreted as neuron's response to the stimuli).
   \end{itemize}
   
   Barlow hypothesized that the spikes in the sensory system formed a neural code for efficiently representing sensory information. \textbf{By efficient, Barlow meant that the code minimized the number of spikes needed to transmit a given signal.} 

\end{frame}



\begin{frame}{Main Question: How would adding be achievable in the Shannon model by means of information theory?}


    Notice that the McCulloch-Pitts  model already USES summation for its threshold logic (\cite{mcculloch_logical_1943})
   
     We know "some kind" of "sum" operation is performed by neurons.
     
     \includegraphics[width=10cm]{images/latexImage_63d4c13d6261c28a5b1ff9f3f41e4f0d.png}

  \textbf{ How is adding possible in the Shannon model by pure means of information theory?}
    
\end{frame}
 




\begin{frame}{Word2vec model: meanings as points (clusters) in multidimensional vector spaces}
    

    Distributional semantics: A word's meaning is given by the words that frequently appear close by (context)

    "You shall know a word by the company it keeps" (J. R. Firth 1957: 11)
    One of the most successful ideas in modern statistical NLP.
    When a word $w$ appears in a text, its context is the set of words that appear nearby (within a fixed-size window).

    Word2vec models (\cite{mikolov2013a}) are in fact weights of neural nets trained to recognize similar words as close vectors in multidimensional vector space.


\end{frame}

\begin{frame}{Traits of compositionality:  Mikolov effect and  analogies of predicates}

   Word models show that meaning (i.e linguistic representation) can be represented as points in multidimensional vector spaces, and whats more, complex meanings can be predicted from simple meanings by means of vector arithmetic (linear algebra)
    Consider the above approximation of meaning relation (\cite{mikolov2013a}):
   \huge $$\text{king} - \text{man} + \text{woman} \approx \text{queen}.$$

\end{frame}

\begin{frame}{Mikolov effect}

   
       
   \textbf{ Mikolov effect suggests that word embeddings space has linear and thus compositional properties}, as we can predict meaning of complex expressions from the meanings of their parts.
  

    What is the explanation of this effect?
    This effect is, I think,  due to  compositionality of meaning in vector spaces, where the meaning of a complex expression can be derived from the meanings of its parts and the way they are combined.
\textbf{
    But what is the scaffolding for this compositionality? How could it be achieved in the Shannon model? }
    
    I propose the construction that partly shows how linearity can be achieved in the Shannon model.

\end{frame}





\begin{frame}{Analogies between predicates: meaning prediction (since \cite{rumelhart_model_1973})}

    \begin{minipage}{0.45\textwidth}
    \begin{tikzpicture}
        % Points
        \coordinate (origin) at (0, 0);
        \coordinate (man) at (0, 0);
        \coordinate (woman) at (1, 1);
        \coordinate (king) at (1, 2);
        \coordinate (queen) at (2, 3);
    
        % Vectors
        \draw[dashed, gray, ->] (man) -- (king);
        \draw[dashed, gray, ->] (woman) -- (queen);
        \draw[black, ->] (man) -- (woman);
        \draw[black, ->] (king) -- (queen);
    
        % Points labels
        \node[below left] at (man) {0};
        \node[below right] at (woman) {woman};
        \node[above left] at (king) {\textbf{king}};
        \node[above right] at (queen) {\textbf{queen}};
        \node[below right] at (man) {man};
    
    \end{tikzpicture}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\textwidth}
    \begin{tikzpicture}
        % Points
        \coordinate (origin) at (0, 0);
        \coordinate (man) at (0, 0);
        \coordinate (woman) at (1, 1);
        \coordinate (king) at (1, 2);
        \coordinate (queen) at (2, 3);
    
        % Vectors
        \draw[dashed, gray, ->] (man) -- (king);
        \draw[dashed, gray, ->] (woman) -- (queen);
        \draw[black, ->] (man) -- (woman);
        \draw[black, ->] (king) -- (queen);
    
        % Points labels
        \node[below left] at (man) {0};
        \node[below right] at (woman) {Woman(x)};
        \node[above left] at (king) {\textbf{King(x)}};
        \node[above right] at (queen) {\textbf{Queen(x)}};
        \node[below right] at (man) {Man(x)};
    
    \end{tikzpicture}
    \end{minipage}
    
   
    
\end{frame}


\begin{frame} {Current state of meaning modelling}
    
   \autocite{potts_case_2019}
   LLMs 
   "



\end{frame}


\begin{frame}{A proposal for Noisy Addition pipeline model }
   
\textbf{How can computaional model acuire linear capabilities by exploting the Shannon model pipeline?}
    
Let me now propose that the "mutated" or "exapted" Shannon communication model  can perform computation: specifically, (noisy) addition without relying on traditional arithmetic operations. Why choose addition? 

Having addition, system could gain the task function of "measuring intensity," which leads to thresholding units (\cite{mcculloch_logical_1943}) and, consequently, to classification capabilities and linear properties in the system.

Consider two channels \(\Gamma_1\) and \(\Gamma_2\) present in an agent—say, a worm \(S\). Together, they transmit signals from the frontal part to the rear motor part of the worm. The signals are transmitted as binary sequences.


\[
S \underset{\text{code}}{\mapsto} A \rightarrow \boxed{\Gamma_1} \rightarrow B \underset{\text{decide}}{\mapsto} \Delta(B) \underset{\text{decode}}{\mapsto} S
\]

\[
S \underset{\text{code}}{\mapsto} A \rightarrow \boxed{\Gamma_2} \rightarrow B \underset{\text{decide}}{\mapsto} \Delta(B) \underset{\text{decode}}{\mapsto} S
\]

\end{frame}

\begin{frame}{A proposal for noisy addition model (cont.) }

\textbf{Now, suppose the channel \(\Gamma_2\) breaks—its receiver fails.} 

The exapted model is now represented as follows:

\[
S \underset{\text{code}}{\mapsto} A \rightarrow \boxed{\Gamma_1} \rightarrow B \underset{\text{decide}}{\mapsto} \Delta(B) \underset{\text{decode}}{\mapsto} S
\]

\[
S \underset{\text{code}}{\mapsto} 
A 
\begin{tikzpicture}[overlay, remember picture]
    \draw[->, red, thick] (0,0) to[out=90,in=200] (0.6,0.9);
\end{tikzpicture}
\textcolor{white}{\rightarrow }
\textcolor{lightgray}{\boxed{\Gamma_2} \rightarrow B \underset{\text{decide}}{\mapsto} \Delta(B) \underset{\text{decode}}{\mapsto} S}
\]

\textbf{Surprisingly, this pipeline is able to perform addition, with given noise. }

\end{frame}


    

\begin{frame}{A proposal for noisy addition model (cont.)}
   
    A plot of outputs of the proposed noisy adder. Depending on noise level it may add better or worse; its main advantage is biological plausability
        
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.7\textwidth]{images/anoisy_adding.png}
            \caption{}
            \label{fig:noisy_adder}
        \end{figure}
    
    
    
    \end{frame}
    


    
    \begin{frame}{Conclusions }
    
        \begin{itemize}
           \item To explain semantic phenomena is to provide a description of the pipeline that processes the variabilty (Carcassi),  what seems like proper scientific task with some prospect of successes.
          
            \item To explain  structural representations, only one kind of information is needed: "Shannon" information. When processed by complex pipelines (e.g., neural networks), it does not become  (1) semantic nor (2) compositional: the whole pipeline has this properties  and qualifies as an S-representation. Our ontology gets healtier. 
            
        \textbf{    c.f. John does not become car-ish when driving a car.}

            \item There is a mechanistic, biologically plausible transition from the original Shannon model to machine learning models such as classifiers (e.g. CNNs). I suggest the minimal improvemnt towards that kind of pipeline is the proposed here "noisy adder".
            
            \item Full, proper explanation of cognitive pipeline is a scientific project of finding the right model of the pipeline that processes the variability (Carcassi) and properly maps it's parts, following 3M rule (\cite{Craver}).
        \end{itemize}
      
    \end{frame}
    
    
    \begin{frame}
        \begin{figure}[h!]
            \centering
            \includegraphics[width=0.8\textwidth]{images/frog_world.png}
            \caption{\Huge Thank you. Represent! }
            
        \end{figure}
      
    \end{frame}
    


    \begin{frame}[label=conclusions2 ]{Conclusions}

    \begin{itemize}
        \item Neural systems can be viewed as pipelines that process variability in the environment, transforming it into structured outputs that guide behavior.
        \item Shannon's model provides a foundational framework for understanding these pipelines, emphasizing the reliable transmission of information.
        \item Semantic information emerges when neural pipelines preserve structural relationships and enable action-guidance, fulfilling the conditions for structural representations.
        \item The gap between Shannon and semantic information can be bridged by extending the Shannon model to include mechanisms like noisy addition, enabling compositionality and structure preservation.
        \item Neural pipelines, such as those in early sensory systems, demonstrate how biological systems implement these principles, mapping variability to structured, actionable outputs.
    \end{itemize}

\end{frame}

 
\begin{frame}

    So, "kind of information" is a misnomer;
    Every time they it is used we mean  the way of measuring inforamtion (by Shannon measure, Fisher measure)  or the properties of the pipelines processing the collections, while the ways of doing it seem to be entirely open.

    

\end{frame}




\begin{frame}[allowframebreaks, label=references]{References}
    \printbibliography
\end{frame}



        \end{document}



% \begin{frame}{Bullets entering one at a time}
%     \begin{itemize}
%       \item Bullet 1
%       \onslide<2->{\item Bullet 2}
%       \onslide<3->{\item Bullet 3}
%       \onslide<4->{\item Bullet 4}
%     \end{itemize}
%   \end{frame}


